{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_wUFHcsm7lM"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IouYJTp5lskH",
        "outputId": "bde79458-9059-4c5d-bcef-f8ad56d8eb81"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: condacolab in /Users/orlandotimmerman/opt/miniconda3/envs/gtc_code/lib/python3.10/site-packages (0.1.5)\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "This module must ONLY run as part of a Colab notebook!",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "File \u001b[0;32m~/opt/miniconda3/envs/gtc_code/lib/python3.10/site-packages/condacolab.py:26\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# install conda (this restarts the colab kernel -- wait until restart before running subsequent blocks)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39mpip install condacolab\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcondacolab\u001b[39;00m\n\u001b[1;32m      5\u001b[0m condacolab\u001b[39m.\u001b[39minstall_miniconda()\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/gtc_code/lib/python3.10/site-packages/condacolab.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mThis module must ONLY run as part of a Colab notebook!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     31\u001b[0m __version__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m0.1.5\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     32\u001b[0m __author__ \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mJaime Rodríguez-Guerra <jaimergp@users.noreply.github.com>\u001b[39m\u001b[39m\"\u001b[39m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: This module must ONLY run as part of a Colab notebook!"
          ]
        }
      ],
      "source": [
        "# install conda (this restarts the colab kernel -- wait until restart before running subsequent blocks)\n",
        "\n",
        "!pip install condacolab\n",
        "import condacolab\n",
        "condacolab.install_miniconda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vs_9wmDtl2Er",
        "outputId": "8a1c5743-aaec-43d8-92d4-f5c890c3a4b3"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas\n",
        "!pip install hyperopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L18wc_7TlxtT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import seaborn as sns\n",
        "import xgboost\n",
        "from pathlib import Path\n",
        "from functools import reduce\n",
        "\n",
        "# from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7HFVokdm9-i"
      },
      "source": [
        "# Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPTxao0rl2cL",
        "outputId": "75d2bd12-c29f-4951-b92d-c0bac3eede53"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'drive' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m\"\u001b[39m\u001b[39m/content/drive/\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'drive' is not defined"
          ]
        }
      ],
      "source": [
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TODO: replace when putting on drive\n",
        "google_drive_personal_key = '/Users/orlandotimmerman/Library/CloudStorage/GoogleDrive-rt582@cam.ac.uk/.shortcut-targets-by-id/132Xl9yWOGKPM7ybLH0oa9c3dJGYrXkjC/'\n",
        "# noaa six-hourly closest to each xbd point\n",
        "df_noaa_xbd_pkl_path = google_drive_personal_key + 'datasets/EFs/weather_data/xbd_obs_noaa_six_hourly.pkl'\n",
        "# xbd observation points\n",
        "df_xbd_points_path = google_drive_personal_key + 'datasets/xBD_data/xbd_points_posthurr_reformatted.pkl'\n",
        "# topographic (flood and storm surge risk, soil properties)\n",
        "df_topographic_efs_path = google_drive_personal_key + 'datasets/processed_data/df_points_posthurr_flood_risk_storm_surge_soil_properties.pkl'\n",
        "# terrain efs\n",
        "df_terrain_efs_path = google_drive_personal_key + 'datasets/processed_data/Terrian_EFs.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_files_in_list_exist(\n",
        "\tfile_list: list[str] | list[Path]\n",
        "\t):\n",
        "\t\"\"\"State which files don't exist and remove from list\"\"\"\n",
        "\tfiles_found = []\n",
        "\tfor fl in file_list:\n",
        "\t\t# attempt conversion to Path object if necessary\n",
        "\t\tif type(fl) != Path:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tfl = Path(fl)\n",
        "\t\t\texcept TypeError:\n",
        "\t\t\t\tprint(f'{fl} could not be converted to Path object')\n",
        "\t\t\n",
        "\t\tif fl.is_file():\n",
        "\t\t\tfiles_found += fl,\n",
        "\t\telse:\n",
        "\t\t\tprint(f'{fl} not found. Removing from list.')\n",
        "\n",
        "\treturn files_found\n",
        "\n",
        "\n",
        "def read_and_merge_pkls(\n",
        "\tpkl_paths: list[str] | list[Path]\n",
        ") -> pd.DataFrame:\n",
        "\t\"\"\"Read in pkl files from list of file paths and merge on index\"\"\"\n",
        "\t# check all files exist\n",
        "\tpkl_paths_present = check_files_in_list_exist(pkl_paths)\n",
        "\tdf_list = [pd.read_pickle(pkl) for pkl in pkl_paths_present]\n",
        "\n",
        "\treturn reduce(lambda df1,df2: pd.merge(df1,df2,left_index=True,right_index=True), df_list)\n",
        "\n",
        "\n",
        "def rename_and_drop_duplicated_cols(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Drop columns which are copies of others and rename the 'asdf_x' headers which would have resulted\"\"\"\n",
        "    # need to ensure no bad types first\n",
        "    df = drop_cols_containing_lists(df)\n",
        "    # remove duplicated columns\n",
        "    dropped_df = df.T.drop_duplicates().T\n",
        "    # rename columns for clarity (especially those which are shared between dfs). Will be able to remove most with better\n",
        "    # column naming further up the process\n",
        "    new_col_names = {col: col.replace('_x', '') for col in dropped_df.columns if col.endswith('_x')}\n",
        "    \n",
        "    return dropped_df.rename(columns=new_col_names)\n",
        "\n",
        "\n",
        "def drop_cols_containing_lists(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"It seemed like the best solution at the time: and to be fair, I can't really think of better...\n",
        "    N.B. for speed, only looks at values in first row – if there is a multi-type column, this would be the least of\n",
        "    our worries...\n",
        "    \"\"\"\n",
        "    df = df.loc[:, df.iloc[0].apply(lambda x: type(x) != list)]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def assign_predictor(\n",
        "    df: pd.DataFrame,\n",
        "    col_name: str,\n",
        "    drop_classes: list[int],\n",
        "    binary_classification: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Assign column as predictor value, and choose whether binary or multi-class classification. Can choose to drop\n",
        "    classes.\"\"\"\n",
        "    df[\"y\"] = df[col_name].astype(int)\n",
        "\n",
        "    if binary_classification:\n",
        "        df.loc[df[\"y\"] > 0, \"y\"] = 1\n",
        "\n",
        "    # drop any classes in \n",
        "    df = df.loc[~df['y'].isin(drop_classes)]\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def replace_cols_with_mean(\n",
        "    df: pd.DataFrame, \n",
        "    col_names: list[str]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Replace values in a column with the mean value\"\"\"\n",
        "    for col in col_names:\n",
        "        df.loc[df[col] == 0, col] = df[col][df[col] > 0].mean()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def train_test_display_model(\n",
        "    df: pd.DataFrame,\n",
        "    var_col_names: list[str],\n",
        "    model_name: str = 'LogisticRegression',\n",
        "    y_col: str = 'y',\n",
        "    test_size: float = 0.25,\n",
        "    random_state: int = 1\n",
        ") -> list:\n",
        "    \"\"\"Specify columns in a df to use to train and test model. Currently available models: 'LogisticRegression', \n",
        "    'RandomForest'\n",
        "\n",
        "    TODO: should I put this in a class?\n",
        "    \"\"\"\n",
        "\n",
        "    x_train, x_test, y_train, y_test = train_test_split(\n",
        "        df[var_col_names], df[y_col], test_size=test_size, random_state=random_state)\n",
        "\n",
        "    # select chosen model\n",
        "    if model_name == 'LogisticRegression':\n",
        "        model = LogisticRegression()\n",
        "        model = train_test_model(model, [x_train, y_train], [x_test, y_test])\n",
        "        importance = model.coef_[0]\n",
        "    elif model_name == 'RandomForest':\n",
        "        model = RandomForestClassifier()\n",
        "        model = train_test_model(model, [x_train, y_train], [x_test, y_test])\n",
        "        importance = model.feature_importances_  \n",
        "\n",
        "    predictions = model.predict(x_test)\n",
        "    # TODO: plot nicely\n",
        "    display(importance)\n",
        "    plot_confusion_matrix(y_test, predictions)\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(\n",
        "    y_test: list,\n",
        "    predictions: list,\n",
        "    score: float,\n",
        "    ax=None\n",
        "):\n",
        "    \"\"\"Plot confusion matrix from y_test and inferred values\"\"\"\n",
        "    damage_labels = {0: 'undamaged', 1: 'minor damage', 2: 'major damage', \n",
        "                      3: 'destroyed', 4: 'unclassified'}\n",
        "\n",
        "    confusion_matrix = metrics.confusion_matrix(y_test, predictions)\n",
        "    # initialise axes if necessary\n",
        "    ax = ax or plt.gca()\n",
        "    sns.heatmap(confusion_matrix/np.sum(confusion_matrix), ax=ax, annot=True, fmt=\".2%\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "    # formatting\n",
        "    ax.set_ylabel('Actual label')\n",
        "    ax.set_xlabel('Predicted label')\n",
        "    # assign integer damage classes to labels\n",
        "    xtick_labels = [damage_labels[i] for i in range(len(confusion_matrix))]\n",
        "    ax.set_xticks(ax.get_xticks(),xtick_labels,rotation=45)\n",
        "    ax.set_yticks(ax.get_yticks(),xtick_labels,rotation=45)\n",
        "    ax.xaxis.set_label_position('top') \n",
        "    ax.xaxis.tick_top()\n",
        "\n",
        "    if len(confusion_matrix) == 2:  # binary classification\n",
        "      ax.set_title(f'Confusion matrix for binary classification \\n Score: {score:.4f}')\n",
        "    else: # multiclass classification\n",
        "      ax.set_title(f'Confusion matrix for multiclass classification \\n Score: {score:.4f}')\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def plot_importances(\n",
        "    var_col_names: list[str],\n",
        "    importances: list[float],\n",
        "    ax=None\n",
        "):\n",
        "    \"\"\"Visualise feature importance\"\"\"\n",
        "    # initialise axes if necessary\n",
        "    ax = ax or plt.gca()\n",
        "    # TODO: add numbers onto bars\n",
        "    ax.barh(var_col_names, importances)\n",
        "    ax.set_ylabel('Input variable')\n",
        "    ax.set_xlabel('Feature importance')\n",
        "    ax.set_title('Feature importance for model')\n",
        "\n",
        "    return ax\n",
        "\n",
        "\n",
        "def train_test_model(\n",
        "    model,\n",
        "    trains: list[list],\n",
        "    tests: list[list]\n",
        ") -> list:\n",
        "    \"\"\"Train provided model. Trains in format [x_train, y_train]; similar with tests\"\"\"\n",
        "    model.fit(trains[0], trains[1])\n",
        "    predictions = model.predict(tests[0])\n",
        "    model.score(tests[0], tests[1])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "pkl_paths = [df_noaa_xbd_pkl_path, df_xbd_points_path, df_topographic_efs_path, df_terrain_efs_path]\n",
        "df_merged = read_and_merge_pkls(pkl_paths)\n",
        "df_merged.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['xbd_obs_geometry', 'damage_class', 'disaster_name', 'capture_date',\n",
              "       'xbd_obs_lon', 'xbd_obs_lat', 'event_start', 'event_end',\n",
              "       'stations_lat_lons', 'noaa_index', 'tag', 'num_entries',\n",
              "       'noaa_obs_date', 'record_id', 'sys_status', 'noaa_obs_lat',\n",
              "       'noaa_obs_lon', 'max_sust_wind', 'min_p', 'r_ne_34', 'r_se_34',\n",
              "       'r_nw_34', 'r_sw_34', 'r_ne_50', 'r_se_50', 'r_nw_50', 'r_sw_50',\n",
              "       'r_ne_64', 'r_se_64', 'r_nw_64', 'r_sw_64', 'strength',\n",
              "       'noaa_obs_geometry', 'shortest_distance_to_track', 'disaster_name',\n",
              "       'flood_risk', 'storm_surge', 'soil_density', 'sand_content',\n",
              "       'clay_content', 'silt_content', 'elevation', 'slope', 'aspect',\n",
              "       'dis2coast'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "out = rename_and_drop_duplicated_cols(df_merged)\n",
        "out.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "var_cols = ['max_sust_wind', 'min_p', 'r_ne_34', 'r_se_34',\n",
        "       'r_nw_34', 'r_sw_34', 'r_ne_50', 'r_se_50', 'r_nw_50', 'r_sw_50',\n",
        "       'r_ne_64', 'r_se_64', 'r_nw_64', 'r_sw_64', 'strength', 'shortest_distance_to_track',\n",
        "       'flood_risk', 'storm_surge', 'soil_density', 'sand_content',\n",
        "       'clay_content', 'silt_content', 'elevation', 'slope', 'aspect',\n",
        "       'dis2coast']\n",
        "\n",
        "# assign target variable\n",
        "df_model_ready = assign_predictor(out,'damage_class',drop_classes=[4],binary_classification=False)\n",
        "\n",
        "# replace necessary columns with mean TODO: ask Ruari about this\n",
        "cols_for_mean = ['soil_density','sand_content','clay_content','silt_content']\n",
        "df_model_ready = replace_cols_with_mean(out,cols_for_mean)\n",
        "    \n",
        "train_test_model(df_model_ready,var_cols,model_name='LogisticRegression')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## xBD Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "priint(f1_score())\n",
        "# F1 score\n",
        "# precision\n",
        "# recall\n",
        "# weighted F1 score\n",
        "# balanced accuracy\n",
        "# PR AUC\n",
        "# ROC AUC"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqJX5FOKnAbr"
      },
      "source": [
        "# Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzFo2LbgmxmT",
        "outputId": "205615bf-efd9-47bc-a614-9ab0bdfdd1ad"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSsKuSoCnZr7"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGK1rrq9nf1j",
        "outputId": "46320692-a7cd-4a73-e1b1-c29dc5f9d348"
      },
      "outputs": [],
      "source": [
        "model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "zeDWujOKpTwD",
        "outputId": "32e364e1-cb9d-442f-f3c5-74eba2663fa7"
      },
      "outputs": [],
      "source": [
        "importance = model.coef_[0]\n",
        "display(importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK65jqJznlcz"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "x4og9UrcnxHy",
        "outputId": "446c6518-3a68-4f25-ade0-522a1e7b7705"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True, fmt=\".2%\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TvFM2Bmg3h"
      },
      "source": [
        "# Random forest hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LxfJ3D_mkSA"
      },
      "outputs": [],
      "source": [
        "# this section is Work In Progress. \n",
        "\n",
        "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
        "\n",
        "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
        "        'max_depth': hp.quniform('max_depth', 10, 1200, 10),\n",
        "        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n",
        "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
        "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
        "        'n_estimators' : hp.choice('n_estimators', [10, 50, 300, 750, 1200,1300,1500])\n",
        "    }\n",
        "\n",
        "def objective(space):\n",
        "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\n",
        "                                 max_features = space['max_features'],\n",
        "                                 min_samples_leaf = space['min_samples_leaf'],\n",
        "                                 min_samples_split = space['min_samples_split'],\n",
        "                                 n_estimators = space['n_estimators'], \n",
        "                                 )\n",
        "    \n",
        "    accuracy = model.score(x_train, y_train)\n",
        "\n",
        "    # We aim to maximize accuracy, therefore we return it as a negative value\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "-JSgENgLmsIT",
        "outputId": "585fe33c-c82d-4881-824d-d4b193a221cc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "trials = Trials()\n",
        "best = fmin(fn= objective,\n",
        "            space= space,\n",
        "            algo= tpe.suggest,\n",
        "            max_evals = 80,\n",
        "            trials= trials)\n",
        "best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqCgKlu3S4Ph"
      },
      "outputs": [],
      "source": [
        "best[\"criterion\"] = \"entropy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7Im_ZhFScB5",
        "outputId": "748b2819-907a-4b85-ecfc-7c8b2f33c430"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(**best)\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51xsrkWam2ic",
        "outputId": "c82b9b12-3b7b-4416-8d03-e2617cfc99dc"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)\n",
        "model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW73u1qlNsBT"
      },
      "outputs": [],
      "source": [
        "importance = best.feature_importances_\n",
        "display(importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdrBxeyfNuCK"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "UfwZQOZUNwZR",
        "outputId": "bbd6bf08-dad1-44c2-b3d2-d1c633926f9f"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True, fmt=\".2%\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB-A7M_2qQ7a"
      },
      "source": [
        "# Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWU5Mf9kqS0q",
        "outputId": "43186482-a298-486a-f2a0-308b81920684"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier()\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xuhcaooqdF6"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfJRNSOuqe4K",
        "outputId": "b56b5e2a-0ac8-4d92-9a1f-d57b0371bc1e"
      },
      "outputs": [],
      "source": [
        "model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ef22PDoSqiha",
        "outputId": "a3fac982-e939-4c08-c25e-43082420cd0c"
      },
      "outputs": [],
      "source": [
        "importance = model.feature_importances_\n",
        "display(importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aw8w0PEqhSq"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "9miSWGT2qiHN",
        "outputId": "22373841-b11f-464f-9c11-cd92d4eeeb6a"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True, fmt=\".2%\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_A0vhalq05K"
      },
      "source": [
        "# XGBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i7Z1RBGq3HJ"
      },
      "outputs": [],
      "source": [
        "model = XGBClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfK9qp9rq-KC",
        "outputId": "7b9a693c-533f-4bb3-9966-217699a0d864"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYc3KvVkq_yJ"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRS9Dv5orOKC",
        "outputId": "735b8733-883e-437b-8a73-1f08fc3bb873"
      },
      "outputs": [],
      "source": [
        "model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KeqoH116rBJi",
        "outputId": "96ca41cb-a0b5-4f66-febf-f9b55340f1a1"
      },
      "outputs": [],
      "source": [
        "importance = model.feature_importances_\n",
        "display(importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV09ckB_rCfJ"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "XTq8yXo3rCx5",
        "outputId": "8f9c979e-513d-462d-8a43-4e0f29f9e290"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True, fmt=\".2%\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gtc_code",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "27e1fd8f4cceca1a6a1bdb538734f1eab8724413a79d947923eebd97498dceb3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
