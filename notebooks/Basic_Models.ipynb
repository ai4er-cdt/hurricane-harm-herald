{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_wUFHcsm7lM"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IouYJTp5lskH",
        "outputId": "bde79458-9059-4c5d-bcef-f8ad56d8eb81"
      },
      "outputs": [],
      "source": [
        "# install conda (this restarts the colab kernel -- wait until restart before running subsequent blocks)\n",
        "\n",
        "!pip install condacolab\n",
        "import condacolab\n",
        "condacolab.install_miniconda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Vs_9wmDtl2Er",
        "outputId": "8a1c5743-aaec-43d8-92d4-f5c890c3a4b3"
      },
      "outputs": [],
      "source": [
        "!pip install geopandas\n",
        "!pip install hyperopt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "L18wc_7TlxtT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "import seaborn as sns\n",
        "import xgboost\n",
        "from pathlib import Path\n",
        "from functools import reduce\n",
        "\n",
        "# from google.colab import drive\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7HFVokdm9-i"
      },
      "source": [
        "# Data Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPTxao0rl2cL",
        "outputId": "75d2bd12-c29f-4951-b92d-c0bac3eede53"
      },
      "outputs": [],
      "source": [
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZk6ulvHmA6j"
      },
      "outputs": [],
      "source": [
        "df = pd.read_pickle(\"/content/drive/MyDrive/ai4er/python/hurricane/hurricane-harm-herald/data/xBD_data/EFs/df_points_posthurr_flood_risk_storm_surge_soil_properties.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "### TODO: replace when putting on drive\n",
        "google_drive_personal_key = '/Users/orlandotimmerman/Library/CloudStorage/GoogleDrive-rt582@cam.ac.uk/.shortcut-targets-by-id/132Xl9yWOGKPM7ybLH0oa9c3dJGYrXkjC/'\n",
        "# noaa six-hourly closest to each xbd point\n",
        "df_noaa_xbd_pkl_path = google_drive_personal_key + 'datasets/EFs/weather_data/xbd_obs_noaa_six_hourly.pkl'\n",
        "# xbd observation points\n",
        "df_xbd_points_path = google_drive_personal_key + 'datasets/xBD_data/xbd_points_posthurr_reformatted.pkl'\n",
        "# topographic (flood and storm surge risk, soil properties)\n",
        "df_topographic_efs_path = google_drive_personal_key + 'datasets/processed_data/df_points_posthurr_flood_risk_storm_surge_soil_properties.pkl'\n",
        "# terrain efs\n",
        "df_terrain_efs_path = google_drive_personal_key + 'datasets/processed_data/Terrian_EFs.pkl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_files_in_list_exist(\n",
        "\tfile_list: list[str] | list[Path]\n",
        "\t):\n",
        "\t\"\"\"State which files don't exist and remove from list\"\"\"\n",
        "\tfiles_found = []\n",
        "\tfor fl in file_list:\n",
        "\t\t# attempt conversion to Path object if necessary\n",
        "\t\tif type(fl) != Path:\n",
        "\t\t\ttry:\n",
        "\t\t\t\tfl = Path(fl)\n",
        "\t\t\texcept TypeError:\n",
        "\t\t\t\tprint(f'{fl} could not be converted to Path object')\n",
        "\t\t\n",
        "\t\tif fl.is_file():\n",
        "\t\t\tfiles_found += fl,\n",
        "\t\telse:\n",
        "\t\t\tprint(f'{fl} not found. Removing from list.')\n",
        "\n",
        "\treturn files_found\n",
        "\n",
        "\n",
        "def read_and_merge_pkls(\n",
        "\tpkl_paths: list[str] | list[Path]\n",
        ") -> pd.DataFrame:\n",
        "\t\"\"\"Read in pkl files from list of file paths and merge on index\"\"\"\n",
        "\t# check all files exist\n",
        "\tpkl_paths_present = check_files_in_list_exist(pkl_paths)\n",
        "\tdf_list = [pd.read_pickle(pkl) for pkl in pkl_paths_present]\n",
        "\n",
        "\treturn reduce(lambda df1,df2: pd.merge(df1,df2,left_index=True,right_index=True), df_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "pkl_paths = [df_noaa_xbd_pkl_path, df_xbd_points_path, df_topographic_efs_path, df_terrain_efs_path]\n",
        "df_merged = read_and_merge_pkls(pkl_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['xbd_obs_geometry', 'damage_class_x', 'disaster_name_x',\n",
              "       'xbd_capture_date', 'xbd_obs_lon', 'xbd_obs_lat', 'event_start',\n",
              "       'event_end', 'closest_stations', 'stations_lat_lons', 'noaa_index',\n",
              "       'tag', 'name', 'num_entries', 'noaa_obs_date', 'record_id',\n",
              "       'sys_status', 'noaa_obs_lat', 'noaa_obs_lon', 'max_sust_wind', 'min_p',\n",
              "       'r_ne_34', 'r_se_34', 'r_nw_34', 'r_sw_34', 'r_ne_50', 'r_se_50',\n",
              "       'r_nw_50', 'r_sw_50', 'r_ne_64', 'r_se_64', 'r_nw_64', 'r_sw_64',\n",
              "       'r_max_wind', 'strength', 'noaa_obs_geometry',\n",
              "       'shortest_distance_to_track', 'geometry_x', 'damage_class_y',\n",
              "       'disaster_name_y', 'capture_date_y', 'lon_x', 'lat_x', 'geometry_y',\n",
              "       'damage_class', 'disaster_name', 'capture_date', 'lat_y', 'lon_y',\n",
              "       'flood_risk', 'storm_surge', 'soil_density', 'sand_content',\n",
              "       'clay_content', 'silt_content', 'latitude', 'longitude', 'geometry',\n",
              "       'elevation', 'slope', 'aspect', 'dis2coast'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_merged.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rename_and_drop_duplicated_cols(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Drop columns which are copies of others and rename the 'asdf_x' headers which would have resulted\"\"\"\n",
        "    # need to ensure no bad types first\n",
        "    df = drop_cols_containing_lists(df)\n",
        "    # remove duplicated columns\n",
        "    dropped_df = df.T.drop_duplicates().T\n",
        "    # rename columns for clarity (especially those which are shared between dfs). Will be able to remove most with better\n",
        "    # column naming further up the process\n",
        "    new_col_names = {col: col.replace('_x', '') for col in dropped_df.columns if col.endswith('_x')}\n",
        "    \n",
        "    return dropped_df.rename(columns=new_col_names)\n",
        "\n",
        "\n",
        "def drop_cols_containing_lists(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"It seemed like the best solution at the time: and to be fair, I can't really think of better...\n",
        "    N.B. for speed, only looks at values in first row – if there is a multi-type column, this would be the least of\n",
        "    our worries...\n",
        "    \"\"\"\n",
        "    df = df.loc[:, df.iloc[0].apply(lambda x: type(x) != list)]    \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "unhashable type: 'list'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m out \u001b[39m=\u001b[39m rename_and_drop_duplicated_cols(df_merged)\n",
            "Cell \u001b[0;32mIn[39], line 8\u001b[0m, in \u001b[0;36mrename_and_drop_duplicated_cols\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      6\u001b[0m df \u001b[39m=\u001b[39m drop_cols_containing_lists(df)\n\u001b[1;32m      7\u001b[0m \u001b[39m# remove duplicated columns\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m dropped_df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mT\u001b[39m.\u001b[39;49mdrop_duplicates()\u001b[39m.\u001b[39mT\n\u001b[1;32m      9\u001b[0m \u001b[39m# rename columns for clarity (especially those which are shared between dfs). Will be able to remove most with better\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# column naming further up the process\u001b[39;00m\n\u001b[1;32m     11\u001b[0m new_col_names \u001b[39m=\u001b[39m {col: col\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m_x\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mfor\u001b[39;00m col \u001b[39min\u001b[39;00m dropped_df\u001b[39m.\u001b[39mcolumns \u001b[39mif\u001b[39;00m col\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m_x\u001b[39m\u001b[39m'\u001b[39m)}\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/gtc_code/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/gtc_code/lib/python3.10/site-packages/pandas/core/frame.py:6672\u001b[0m, in \u001b[0;36mDataFrame.drop_duplicates\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6670\u001b[0m inplace \u001b[39m=\u001b[39m validate_bool_kwarg(inplace, \u001b[39m\"\u001b[39m\u001b[39minplace\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6671\u001b[0m ignore_index \u001b[39m=\u001b[39m validate_bool_kwarg(ignore_index, \u001b[39m\"\u001b[39m\u001b[39mignore_index\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 6672\u001b[0m duplicated \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mduplicated(subset, keep\u001b[39m=\u001b[39;49mkeep)\n\u001b[1;32m   6674\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m[\u001b[39m-\u001b[39mduplicated]\n\u001b[1;32m   6675\u001b[0m \u001b[39mif\u001b[39;00m ignore_index:\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/gtc_code/lib/python3.10/site-packages/pandas/core/frame.py:6814\u001b[0m, in \u001b[0;36mDataFrame.duplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   6812\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6813\u001b[0m     vals \u001b[39m=\u001b[39m (col\u001b[39m.\u001b[39mvalues \u001b[39mfor\u001b[39;00m name, col \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m subset)\n\u001b[0;32m-> 6814\u001b[0m     labels, shape \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlist\u001b[39m, \u001b[39mzip\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39mmap\u001b[39;49m(f, vals)))\n\u001b[1;32m   6816\u001b[0m     ids \u001b[39m=\u001b[39m get_group_index(\n\u001b[1;32m   6817\u001b[0m         labels,\n\u001b[1;32m   6818\u001b[0m         \u001b[39m# error: Argument 1 to \"tuple\" has incompatible type \"List[_T]\";\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6822\u001b[0m         xnull\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   6823\u001b[0m     )\n\u001b[1;32m   6824\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_constructor_sliced(duplicated(ids, keep), index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/gtc_code/lib/python3.10/site-packages/pandas/core/frame.py:6782\u001b[0m, in \u001b[0;36mDataFrame.duplicated.<locals>.f\u001b[0;34m(vals)\u001b[0m\n\u001b[1;32m   6781\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mf\u001b[39m(vals) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[np\u001b[39m.\u001b[39mndarray, \u001b[39mint\u001b[39m]:\n\u001b[0;32m-> 6782\u001b[0m     labels, shape \u001b[39m=\u001b[39m algorithms\u001b[39m.\u001b[39;49mfactorize(vals, size_hint\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m))\n\u001b[1;32m   6783\u001b[0m     \u001b[39mreturn\u001b[39;00m labels\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mi8\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m), \u001b[39mlen\u001b[39m(shape)\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/gtc_code/lib/python3.10/site-packages/pandas/core/algorithms.py:822\u001b[0m, in \u001b[0;36mfactorize\u001b[0;34m(values, sort, na_sentinel, use_na_sentinel, size_hint)\u001b[0m\n\u001b[1;32m    819\u001b[0m             \u001b[39m# Don't modify (potentially user-provided) array\u001b[39;00m\n\u001b[1;32m    820\u001b[0m             values \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(null_mask, na_value, values)\n\u001b[0;32m--> 822\u001b[0m     codes, uniques \u001b[39m=\u001b[39m factorize_array(\n\u001b[1;32m    823\u001b[0m         values,\n\u001b[1;32m    824\u001b[0m         na_sentinel\u001b[39m=\u001b[39;49mna_sentinel_arg,\n\u001b[1;32m    825\u001b[0m         size_hint\u001b[39m=\u001b[39;49msize_hint,\n\u001b[1;32m    826\u001b[0m     )\n\u001b[1;32m    828\u001b[0m \u001b[39mif\u001b[39;00m sort \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    829\u001b[0m     \u001b[39mif\u001b[39;00m na_sentinel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    830\u001b[0m         \u001b[39m# TODO: Can remove when na_sentinel=na_sentinel as in TODO above\u001b[39;00m\n",
            "File \u001b[0;32m~/opt/miniconda3/envs/gtc_code/lib/python3.10/site-packages/pandas/core/algorithms.py:578\u001b[0m, in \u001b[0;36mfactorize_array\u001b[0;34m(values, na_sentinel, size_hint, na_value, mask)\u001b[0m\n\u001b[1;32m    575\u001b[0m hash_klass, values \u001b[39m=\u001b[39m _get_hashtable_algo(values)\n\u001b[1;32m    577\u001b[0m table \u001b[39m=\u001b[39m hash_klass(size_hint \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(values))\n\u001b[0;32m--> 578\u001b[0m uniques, codes \u001b[39m=\u001b[39m table\u001b[39m.\u001b[39;49mfactorize(\n\u001b[1;32m    579\u001b[0m     values,\n\u001b[1;32m    580\u001b[0m     na_sentinel\u001b[39m=\u001b[39;49mna_sentinel,\n\u001b[1;32m    581\u001b[0m     na_value\u001b[39m=\u001b[39;49mna_value,\n\u001b[1;32m    582\u001b[0m     mask\u001b[39m=\u001b[39;49mmask,\n\u001b[1;32m    583\u001b[0m     ignore_na\u001b[39m=\u001b[39;49mignore_na,\n\u001b[1;32m    584\u001b[0m )\n\u001b[1;32m    586\u001b[0m \u001b[39m# re-cast e.g. i8->dt64/td64, uint8->bool\u001b[39;00m\n\u001b[1;32m    587\u001b[0m uniques \u001b[39m=\u001b[39m _reconstruct_data(uniques, original\u001b[39m.\u001b[39mdtype, original)\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5943\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.factorize\u001b[0;34m()\u001b[0m\n",
            "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5857\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable._unique\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
          ]
        }
      ],
      "source": [
        "out = rename_and_drop_duplicated_cols(df_merged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# remove cols containing lists (closest weather stations)\n",
        "dropped_df = df_merged.drop(columns=['closest_stations','stations_lat_lons'],axis=1)\n",
        "# remove duplicated columns\n",
        "dropped_df = dropped_df.T.drop_duplicates().T\n",
        "# rename columns for clarity (especially those which are shared between dfs). Will be able to remove most with better\n",
        "# column naming further up the process\n",
        "new_col_names = {col: col.replace('_x', '') for col in dropped_df.columns if col.endswith('_x')}\n",
        "df = dropped_df.rename(columns=new_col_names)\n",
        "\n",
        "\n",
        "# cols_name_mapping = {\n",
        "#     'capture_date_x': 'xbd_capture_date'\n",
        "# }\n",
        "# df_merged.rename(columns=cols_name_mapping,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['xbd_obs_geometry', 'damage_class', 'disaster_name', 'xbd_capture_date',\n",
              "       'xbd_obs_lon', 'xbd_obs_lat', 'event_start', 'event_end', 'noaa_index',\n",
              "       'tag', 'num_entries', 'noaa_obs_date', 'record_id', 'sys_status',\n",
              "       'noaa_obs_lat', 'noaa_obs_lon', 'max_sust_wind', 'min_p', 'r_ne_34',\n",
              "       'r_se_34', 'r_nw_34', 'r_sw_34', 'r_ne_50', 'r_se_50', 'r_nw_50',\n",
              "       'r_sw_50', 'r_ne_64', 'r_se_64', 'r_nw_64', 'r_sw_64', 'r_max_wind',\n",
              "       'strength', 'noaa_obs_geometry', 'shortest_distance_to_track',\n",
              "       'disaster_name', 'flood_risk', 'storm_surge', 'soil_density',\n",
              "       'sand_content', 'clay_content', 'silt_content', 'elevation', 'slope',\n",
              "       'aspect', 'dis2coast'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPXD548xmEl7"
      },
      "outputs": [],
      "source": [
        "df[\"y\"] = df[\"damage_class\"]\n",
        "df[\"y\"] = df[\"y\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGW6ea7jkJjx"
      },
      "outputs": [],
      "source": [
        "# run this if you want to do binary classification\n",
        "df.loc[df[\"y\"] > 0, \"y\"] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6lg6QovfXyJ"
      },
      "outputs": [],
      "source": [
        "def replace_with_mean(df: pd.core.frame.DataFrame, column: str):\n",
        "    df.loc[df[column] == 0, column] = df[column][df[column] > 0].mean()\n",
        "\n",
        "replace_with_mean(df, \"soil_density\")\n",
        "replace_with_mean(df, \"sand_content\")\n",
        "replace_with_mean(df, \"clay_content\")\n",
        "replace_with_mean(df, \"silt_content\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ngzh0Y0Cm60T"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df[[\"storm_surge\", \"flood_risk\", \"soil_density\", \"sand_content\", \"clay_content\", \n",
        "                                                        \"silt_content\"]],\n",
        "                                                    df[\"y\"], test_size=0.25, random_state=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps7GdpQJat4m"
      },
      "source": [
        "# Add weather data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nr2kylPSa0G_"
      },
      "outputs": [],
      "source": [
        "df_weather = pd.read_pickle(\"/content/drive/MyDrive/ai4er/python/hurricane/hurricane-harm-herald/data/xBD_data/weather_data/xbd_obs_stations.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 866
        },
        "id": "0dI818wTbsxO",
        "outputId": "3b8bd9d6-0181-436b-8936-d2ca8069e94a"
      },
      "outputs": [],
      "source": [
        "df_weather"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqJX5FOKnAbr"
      },
      "source": [
        "# Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzFo2LbgmxmT",
        "outputId": "205615bf-efd9-47bc-a614-9ab0bdfdd1ad"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSsKuSoCnZr7"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGK1rrq9nf1j",
        "outputId": "46320692-a7cd-4a73-e1b1-c29dc5f9d348"
      },
      "outputs": [],
      "source": [
        "model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "zeDWujOKpTwD",
        "outputId": "32e364e1-cb9d-442f-f3c5-74eba2663fa7"
      },
      "outputs": [],
      "source": [
        "importance = model.coef_[0]\n",
        "display(importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK65jqJznlcz"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "x4og9UrcnxHy",
        "outputId": "446c6518-3a68-4f25-ade0-522a1e7b7705"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True, fmt=\".2%\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TvFM2Bmg3h"
      },
      "source": [
        "# Random forest hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LxfJ3D_mkSA"
      },
      "outputs": [],
      "source": [
        "# this section is Work In Progress. \n",
        "\n",
        "from hyperopt import hp,fmin,tpe,STATUS_OK,Trials\n",
        "\n",
        "space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n",
        "        'max_depth': hp.quniform('max_depth', 10, 1200, 10),\n",
        "        'max_features': hp.choice('max_features', ['auto', 'sqrt','log2', None]),\n",
        "        'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
        "        'min_samples_split' : hp.uniform ('min_samples_split', 0, 1),\n",
        "        'n_estimators' : hp.choice('n_estimators', [10, 50, 300, 750, 1200,1300,1500])\n",
        "    }\n",
        "\n",
        "def objective(space):\n",
        "    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\n",
        "                                 max_features = space['max_features'],\n",
        "                                 min_samples_leaf = space['min_samples_leaf'],\n",
        "                                 min_samples_split = space['min_samples_split'],\n",
        "                                 n_estimators = space['n_estimators'], \n",
        "                                 )\n",
        "    \n",
        "    accuracy = model.score(x_train, y_train)\n",
        "\n",
        "    # We aim to maximize accuracy, therefore we return it as a negative value\n",
        "    return {'loss': -accuracy, 'status': STATUS_OK }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "-JSgENgLmsIT",
        "outputId": "585fe33c-c82d-4881-824d-d4b193a221cc"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "trials = Trials()\n",
        "best = fmin(fn= objective,\n",
        "            space= space,\n",
        "            algo= tpe.suggest,\n",
        "            max_evals = 80,\n",
        "            trials= trials)\n",
        "best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fqCgKlu3S4Ph"
      },
      "outputs": [],
      "source": [
        "best[\"criterion\"] = \"entropy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7Im_ZhFScB5",
        "outputId": "748b2819-907a-4b85-ecfc-7c8b2f33c430"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier(**best)\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51xsrkWam2ic",
        "outputId": "c82b9b12-3b7b-4416-8d03-e2617cfc99dc"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)\n",
        "model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LW73u1qlNsBT"
      },
      "outputs": [],
      "source": [
        "importance = best.feature_importances_\n",
        "display(importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdrBxeyfNuCK"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "UfwZQOZUNwZR",
        "outputId": "bbd6bf08-dad1-44c2-b3d2-d1c633926f9f"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True, fmt=\".2%\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB-A7M_2qQ7a"
      },
      "source": [
        "# Random Forest Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWU5Mf9kqS0q",
        "outputId": "43186482-a298-486a-f2a0-308b81920684"
      },
      "outputs": [],
      "source": [
        "model = RandomForestClassifier()\n",
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xuhcaooqdF6"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfJRNSOuqe4K",
        "outputId": "b56b5e2a-0ac8-4d92-9a1f-d57b0371bc1e"
      },
      "outputs": [],
      "source": [
        "model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ef22PDoSqiha",
        "outputId": "a3fac982-e939-4c08-c25e-43082420cd0c"
      },
      "outputs": [],
      "source": [
        "importance = model.feature_importances_\n",
        "display(importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aw8w0PEqhSq"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "9miSWGT2qiHN",
        "outputId": "22373841-b11f-464f-9c11-cd92d4eeeb6a"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True, fmt=\".2%\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_A0vhalq05K"
      },
      "source": [
        "# XGBoost Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i7Z1RBGq3HJ"
      },
      "outputs": [],
      "source": [
        "model = XGBClassifier()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfK9qp9rq-KC",
        "outputId": "7b9a693c-533f-4bb3-9966-217699a0d864"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYc3KvVkq_yJ"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRS9Dv5orOKC",
        "outputId": "735b8733-883e-437b-8a73-1f08fc3bb873"
      },
      "outputs": [],
      "source": [
        "model.score(x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KeqoH116rBJi",
        "outputId": "96ca41cb-a0b5-4f66-febf-f9b55340f1a1"
      },
      "outputs": [],
      "source": [
        "importance = model.feature_importances_\n",
        "display(importance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uV09ckB_rCfJ"
      },
      "outputs": [],
      "source": [
        "confusion_matrix = metrics.confusion_matrix(y_test, predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "XTq8yXo3rCx5",
        "outputId": "8f9c979e-513d-462d-8a43-4e0f29f9e290"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(confusion_matrix/np.sum(confusion_matrix), annot=True, fmt=\".2%\", linewidths=.5, square = True, cmap = 'Blues_r')\n",
        "plt.ylabel('Actual label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gtc_code",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "27e1fd8f4cceca1a6a1bdb538734f1eab8724413a79d947923eebd97498dceb3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
