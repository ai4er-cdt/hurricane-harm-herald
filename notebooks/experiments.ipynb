{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Installs (this restarts the kernel)"
   ],
   "metadata": {
    "id": "vSPd4Nrx8cl6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install pytorch_lightning\n",
    "!pip install geopandas\n",
    "!pip install pandas --upgrade\n",
    "!pip install rich --upgrade\n",
    "!pip install timm"
   ],
   "metadata": {
    "id": "dhNMg5iUbWQk"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# runtime has to restart to avoid an error in the following imports\n",
    "exit()"
   ],
   "metadata": {
    "id": "3-EBEeL08V7Z"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "id": "nrFsJ8Th4A9B"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "metadata": {
    "id": "fcMOeGgZa1i0"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "repo_name = \"hurricane-harm-herald\"\n",
    "target_dir = \"/content/\"\n",
    "os.chdir(target_dir)\n",
    "print(os.getcwd())"
   ],
   "metadata": {
    "id": "g-zCuGeXa397",
    "outputId": "6cc25fc4-c26a-4cdd-dcd6-5763d3f4dae8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "os.system(\"git clone --branch dev https://@github.com/ai4er-cdt/{}\".format(repo_name))"
   ],
   "metadata": {
    "id": "BW2nWY_Wa5fz",
    "outputId": "9f881545-7097-45ab-c950-540602f2b299",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%cd hurricane-harm-herald"
   ],
   "metadata": {
    "id": "zux6jK_Za_Sc",
    "outputId": "385cbd82-f849-44e7-eae6-5c60a715b4da",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/hurricane-harm-herald\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from h3.dataprocessing.DataAugmentation import DataAugmentation\n",
    "from h3.dataloading.HurricaneDataset import HurricaneDataset\n",
    "from h3.models.multimodal import OverallModel\n",
    "from h3.models.balance_process import main as balance_process_main"
   ],
   "metadata": {
    "id": "eyu7EGgV8OLR"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Loading Functions"
   ],
   "metadata": {
    "id": "68VADAYU4JZg"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "## Call function from basic models ipynb\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "from functools import reduce\n",
    "\n",
    "def check_files_in_list_exist(\n",
    "    file_list: Union[List[str], List[Path]]\n",
    "    ):\n",
    "    \"\"\"State which files don't exist and remove from list\"\"\"\n",
    "    files_found = []\n",
    "    for fl in file_list:\n",
    "        # attempt conversion to Path object if necessary\n",
    "        if type(fl) != Path:\n",
    "            try:\n",
    "                fl = Path(fl)\n",
    "            except TypeError:\n",
    "                print(f'{fl} could not be converted to Path object')\n",
    "        \n",
    "        if fl.is_file():\n",
    "            files_found += fl,\n",
    "        else:\n",
    "            print(f'{fl} not found. Removing from list.')\n",
    "\n",
    "    return files_found\n",
    "\n",
    "\n",
    "def read_and_merge_pkls(\n",
    "    pkl_paths: Union[List[str], List[Path]]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Read in pkl files from list of file paths and merge on index\"\"\"\n",
    "    # check all files exist\n",
    "    pkl_paths_present = check_files_in_list_exist(pkl_paths)\n",
    "    df_list = [pd.read_pickle(pkl) for pkl in pkl_paths_present]\n",
    "\n",
    "    return reduce(lambda df1,df2: pd.merge(df1,df2,left_index=True,right_index=True), df_list)\n",
    "\n",
    "\n",
    "def rename_and_drop_duplicated_cols(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Drop columns which are copies of others and rename the 'asdf_x' headers which would have resulted\"\"\"\n",
    "    # need to ensure no bad types first\n",
    "    df = drop_cols_containing_lists(df)\n",
    "    # remove duplicated columns\n",
    "    dropped_df = df.T.drop_duplicates().T\n",
    "    # rename columns for clarity (especially those which are shared between dfs). Will be able to remove most with better\n",
    "    # column naming further up the process\n",
    "    new_col_names = {col: col.replace('_x', '') for col in dropped_df.columns if col.endswith('_x')}\n",
    "    \n",
    "    return dropped_df.rename(columns=new_col_names)\n",
    "\n",
    "def drop_cols_containing_lists(\n",
    "    df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"It seemed like the best solution at the time: and to be fair, I can't really think of better...\n",
    "    N.B. for speed, only looks at values in first row â€“ if there is a multi-type column, this would be the least of\n",
    "    our worries...\n",
    "    \"\"\"\n",
    "    df = df.loc[:, df.iloc[0].apply(lambda x: type(x) != list)]\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "id": "yWAhcwSObBg0"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load dataframes, setup directories"
   ],
   "metadata": {
    "id": "e4i_Y4l14V7A"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# note: the drive paths may be different for you (for some reason \"datasets\" is \"xBD_data\" for me)\n",
    "drive.mount(\"/content/drive/\")\n",
    "\n",
    "data_dir = \"/content/drive/MyDrive/ai4er/python/hurricane/hurricane-harm-herald/data/datasets/\""
   ],
   "metadata": {
    "id": "lG8Du2-AbQZ8",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b1d8ddd5-e25f-4c28-e90c-40386f260cf1"
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Mounted at /content/drive/\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# again, may need to change \"xBD_data\" to \"datasets\"\n",
    "\n",
    "!cp -r /content/drive/MyDrive/ai4er/python/hurricane/hurricane-harm-herald/data/datasets/processed_data/processed_xbd/geotiffs_zoom/images/zoom_05.tar.gz /content\n",
    "!cp -r /content/drive/MyDrive/ai4er/python/hurricane/hurricane-harm-herald/data/datasets/processed_data/processed_xbd/geotiffs_zoom/images/zoom_1.tar.gz /content\n",
    "!cp -r /content/drive/MyDrive/ai4er/python/hurricane/hurricane-harm-herald/data/datasets/processed_data/processed_xbd/geotiffs_zoom/images/zoom_2.tar.gz /content\n",
    "!cp -r /content/drive/MyDrive/ai4er/python/hurricane/hurricane-harm-herald/data/datasets/processed_data/processed_xbd/geotiffs_zoom/images/zoom_4.tar.gz /content"
   ],
   "metadata": {
    "id": "zySO0165bjf8"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir /content/images\n",
    "!mkdir /content/checkpoints"
   ],
   "metadata": {
    "id": "szUUF83SABfS"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!tar -xzf /content/zoom_05.tar.gz -C /content/images/\n",
    "!rm /content/zoom_05.tar.gz\n",
    "\n",
    "!tar -xzf /content/zoom_1.tar.gz -C /content/images/\n",
    "!rm /content/zoom_1.tar.gz\n",
    "\n",
    "!tar -xzf /content/zoom_2.tar.gz -C /content/images/\n",
    "!rm /content/zoom_2.tar.gz\n",
    "\n",
    "!tar -xzf /content/zoom_4.tar.gz -C /content/images/\n",
    "!rm /content/zoom_4.tar.gz\n"
   ],
   "metadata": {
    "id": "x3Qra9H2cX40"
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# the below directory should be to the .pkl with all EFs\n",
    "img_path = \"/content/images/\""
   ],
   "metadata": {
    "id": "Iee0aDVqcYsc"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "df.columns"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F19i6MyEEbj0",
    "outputId": "f75499c2-65ce-45a9-f532-21e5af3aac67"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['xbd_obs_geometry', 'polygon_lnglat', 'pointy', 'polygony',\n",
       "       'disaster_name', 'image_name', 'capture_date', 'json_link',\n",
       "       'damage_class', 'xbd_obs_lon', 'xbd_obs_lat', 'noaa_index', 'tag',\n",
       "       'num_entries', 'noaa_obs_date', 'record_id', 'sys_status',\n",
       "       'noaa_obs_lat', 'noaa_obs_lon', 'max_sust_wind', 'min_p', 'r_ne_34',\n",
       "       'r_se_34', 'r_nw_34', 'r_sw_34', 'r_ne_50', 'r_se_50', 'r_nw_50',\n",
       "       'r_sw_50', 'r_ne_64', 'r_se_64', 'r_nw_64', 'r_sw_64', 'r_max_wind',\n",
       "       'strength', 'noaa_obs_geometry', 'shortest_distance_to_track',\n",
       "       'disaster_name_y', 'storm_surge', 'soil_density', 'sand_content',\n",
       "       'clay_content', 'silt_content', 'elevation', 'slope', 'aspect',\n",
       "       'dis2coast', 'id'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "filtered_pickle_path = os.path.join(data_dir, \"processed_data/metadata_pickle/filtered_lnglat_pre_pol_post_damage.pkl\")\n",
    "\n",
    "if os.path.exists(filtered_pickle_path):\n",
    "    balanced_df = pd.read_pickle(filtered_pickle_path)\n",
    "else:\n",
    "    balanced_df = balance_process_main(data_dir)\n",
    "\n",
    "# remove unclassified class\n",
    "balanced_df = balanced_df[balanced_df.damage_class != 4]\n",
    "balanced_df[\"id\"] = balanced_df.index"
   ],
   "metadata": {
    "id": "-Xc-DmaWK71C"
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Choose EFs, train/val/test split, run model"
   ],
   "metadata": {
    "id": "MdflzOhtFXDd"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# maybe unwise to use all features\n",
    "# use RF feature importance and pick best features\n",
    "# n.b. r_max_wind is NaN so don't use\n",
    "\n",
    "EF_features = {\n",
    "\t\t\"weather\": [\n",
    "\t\t\t\"max_sust_wind\", \"shortest_distance_to_track\", \"min_p\",\n",
    "\t\t\t\"r_nw_34\", \"r_sw_34\",\n",
    "\t\t],\n",
    "\t\t\"soil\": [\"soil_density\", \"sand_content\", \"clay_content\", \"silt_content\"],\n",
    "\t\t\"storm_surge\": [\"storm_surge\"],\n",
    "\t\t\"dem\": [\"elevation\", \"slope\", \"aspect\", \"dis2coast\"]}"
   ],
   "metadata": {
    "id": "_b79z_nhcZuk"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# insert Lisannes df downsampling code here \n",
    "\n",
    "# consider using a set seed for consistency\n",
    "train_df, test_df = train_test_split(balanced_df, test_size = 0.1, random_state = 1)\n",
    "train_df, val_df = train_test_split(train_df, test_size = 0.2/0.9, random_state = 1)"
   ],
   "metadata": {
    "id": "HvVKlpRecbVM"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "features_to_scale = [\n",
    "\t\t\"max_sust_wind\", \"shortest_distance_to_track\", \"min_p\",\n",
    "\t\t\"r_nw_34\", \"r_sw_34\",\n",
    "\t\t\"soil_density\", \"sand_content\", \"clay_content\", \"silt_content\",\n",
    "\t\t\"storm_surge\",\n",
    "\t\t\"elevation\", \"slope\", \"aspect\", \"dis2coast\"\n",
    "\t]\n",
    "\n",
    "scaled_train_df = train_df.copy()\n",
    "scaled_val_df = val_df.copy()\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_train_df[features_to_scale] = scaler.fit_transform(scaled_train_df[features_to_scale])\n",
    "scaled_val_df[features_to_scale] = scaler.transform(val_df[features_to_scale])"
   ],
   "metadata": {
    "id": "qLqJ2_B8VA_p"
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#augmentations = DataAugmentation()\n",
    "augmentations = DataAugmentation()"
   ],
   "metadata": {
    "id": "WY2JDs5qckus"
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# zoom_levels = [\"1\", \"2\", \"4\", \"0.5\"]\n",
    "zoom_levels = [\"1\"]\n",
    "image_embedding_architecture = \"SatMAE\"\n",
    "\n",
    "# class weights for weighted cross-entropy loss\n",
    "# class_weights = compute_class_weight(class_weight = \"balanced\",\n",
    "#                                      classes = np.unique(train_df[\"damage_class\"].to_numpy()),\n",
    "#                                      y = train_df[\"damage_class\"])\n",
    "\n",
    "# class_weights = torch.as_tensor(class_weights).type(torch.FloatTensor)\n",
    "\n",
    "\n",
    "train_dataset = HurricaneDataset(scaled_train_df, img_path, EF_features,\n",
    "                                 image_embedding_architecture = image_embedding_architecture,\n",
    "                                 zoom_levels = zoom_levels,\n",
    "                                 augmentations = augmentations)\n",
    "\n",
    "val_dataset = HurricaneDataset(scaled_val_df, img_path, EF_features,\n",
    "                               image_embedding_architecture = image_embedding_architecture,\n",
    "                               zoom_levels = zoom_levels)\n",
    "\n",
    "if cuda_device:\n",
    "    torch.set_float32_matmul_precision('medium')\n",
    "    num_workers = 4\n",
    "    persistent_w = bool(num_workers)\n",
    "else:\n",
    "    num_workers = 0\n",
    "    persistent_w = False\n",
    "\n",
    "model = OverallModel(training_dataset = train_dataset, \n",
    "                     validation_dataset = val_dataset,\n",
    "                     num_input_channels = 3,\n",
    "                     EF_features = EF_features,\n",
    "                     batch_size = 64,\n",
    "                     image_embedding_architecture = image_embedding_architecture,\n",
    "                     image_encoder_lr = 0,\n",
    "                     general_lr = 1e-3,\n",
    "                     output_activation = None,\n",
    "                     loss_function_str = \"CELoss\",\n",
    "                     num_output_classes = 4,\n",
    "                     lr_scheduler_patience = 3,\n",
    "                     zoom_levels = zoom_levels,\n",
    "                     #class_weights = class_weights,\n",
    "                     image_only_model = False,\n",
    "                     weight_decay = 0.001,\n",
    "                     num_workers = num_workers,\n",
    "                     persistent_w = persistent_w)\n",
    "\n",
    "max_epochs = 1\n",
    "log_every_n_steps = 100 \n",
    "\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val/loss\", patience=5, mode=\"min\")\n",
    "\n",
    "experiment_name = \"experiment_test1\"\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val/loss\",\n",
    "        dirpath=os.path.join(data_dir, \"checkpoints\", experiment_name),\n",
    "        filename=\"{epoch}-{val/loss:.4f}\",\n",
    "        save_top_k=1,        # save the best model\n",
    "        mode=\"min\",\n",
    "        every_n_epochs=1\n",
    "    )\n",
    "\n",
    "tic = time.perf_counter()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    trainer = pl.Trainer(max_epochs = max_epochs, accelerator = 'gpu',\n",
    "                         log_every_n_steps = log_every_n_steps,\n",
    "                         callbacks = [checkpoint_callback, early_stop_callback],\n",
    "                         profiler = \"simple\")\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs = max_epochs,\n",
    "                         log_every_n_steps = log_every_n_steps,\n",
    "                         callbacks = [checkpoint_callback, early_stop_callback],\n",
    "                         profiler = \"simple\")\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=lightning_logs/\n",
    "trainer.fit(model)\n",
    "\n",
    "toc = time.perf_counter()\n",
    "display(toc - tic)"
   ],
   "metadata": {
    "id": "IcMe-NhodBeW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load a model from a saved checkpoint"
   ],
   "metadata": {
    "id": "NvurM5L_TCAp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# having to put in training_dataset and validation_dataset is weird\n",
    "model_from_ckpt = OverallModel.load_from_checkpoint(\"/content/checkpoints/epoch=0-val/loss=6.16.ckpt\",\n",
    "                                                    training_dataset = train_dataset,\n",
    "                                                    validation_dataset = val_dataset)\n"
   ],
   "metadata": {
    "id": "xznnrliHQoEg"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "gpuClass": "standard",
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
