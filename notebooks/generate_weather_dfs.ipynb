{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtc_functions\n",
    "# having issues with circular dependencies here\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "\n",
    "# remove these once scripts have been transferred\n",
    "from tqdm import tqdm\n",
    "from typing import Tuple,List,Union\n",
    "import os\n",
    "import numpy as np\n",
    "import urllib\n",
    "import requests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading in weather station data\n",
    "\n",
    "Load in list of weather stations. This file can be found at the bottom of this [webpage](https://www.ncei.noaa.gov/pub/data/noaa/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: better common path solution. Lisanne's use of Owen's functions? Cambridge-hosted data server?\n",
    "google_drive_personal_key = '/Users/orlandotimmerman/Library/CloudStorage/GoogleDrive-rt582@cam.ac.uk/.shortcut-targets-by-id/132Xl9yWOGKPM7ybLH0oa9c3dJGYrXkjC/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>usaf</th>\n",
       "      <th>wban</th>\n",
       "      <th>station_name</th>\n",
       "      <th>ctry</th>\n",
       "      <th>state</th>\n",
       "      <th>icao</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>elev(m)</th>\n",
       "      <th>begin</th>\n",
       "      <th>end</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007018</td>\n",
       "      <td>99999</td>\n",
       "      <td>WXPOD 7018</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7018.0</td>\n",
       "      <td>2011-03-09</td>\n",
       "      <td>2013-07-30</td>\n",
       "      <td>POINT (0 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>007026</td>\n",
       "      <td>99999</td>\n",
       "      <td>WXPOD 7026</td>\n",
       "      <td>AF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7026.0</td>\n",
       "      <td>2012-07-13</td>\n",
       "      <td>2017-08-22</td>\n",
       "      <td>POINT (0 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>007070</td>\n",
       "      <td>99999</td>\n",
       "      <td>WXPOD 7070</td>\n",
       "      <td>AF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>7070.0</td>\n",
       "      <td>2014-09-23</td>\n",
       "      <td>2015-09-26</td>\n",
       "      <td>POINT (0 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>008260</td>\n",
       "      <td>99999</td>\n",
       "      <td>WXPOD8270</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2005-01-01</td>\n",
       "      <td>2012-07-31</td>\n",
       "      <td>POINT (0 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>008268</td>\n",
       "      <td>99999</td>\n",
       "      <td>WXPOD8278</td>\n",
       "      <td>AF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.95</td>\n",
       "      <td>65.567</td>\n",
       "      <td>1156.7</td>\n",
       "      <td>2010-05-19</td>\n",
       "      <td>2012-03-23</td>\n",
       "      <td>POINT (65.567 32.95)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>008307</td>\n",
       "      <td>99999</td>\n",
       "      <td>WXPOD 8318</td>\n",
       "      <td>AF</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>8318.0</td>\n",
       "      <td>2010-04-21</td>\n",
       "      <td>2010-04-21</td>\n",
       "      <td>POINT (0 0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>008411</td>\n",
       "      <td>99999</td>\n",
       "      <td>XM20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>POINT EMPTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>008414</td>\n",
       "      <td>99999</td>\n",
       "      <td>XM18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-02-16</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>POINT EMPTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>008415</td>\n",
       "      <td>99999</td>\n",
       "      <td>XM21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>2020-03-14</td>\n",
       "      <td>POINT EMPTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008418</td>\n",
       "      <td>99999</td>\n",
       "      <td>XM24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>2016-02-17</td>\n",
       "      <td>POINT EMPTY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     usaf   wban station_name ctry state icao    lat     lon  elev(m)  \\\n",
       "0  007018  99999   WXPOD 7018  NaN   NaN  NaN   0.00   0.000   7018.0   \n",
       "1  007026  99999   WXPOD 7026   AF   NaN  NaN   0.00   0.000   7026.0   \n",
       "2  007070  99999   WXPOD 7070   AF   NaN  NaN   0.00   0.000   7070.0   \n",
       "3  008260  99999    WXPOD8270  NaN   NaN  NaN   0.00   0.000      0.0   \n",
       "4  008268  99999    WXPOD8278   AF   NaN  NaN  32.95  65.567   1156.7   \n",
       "5  008307  99999   WXPOD 8318   AF   NaN  NaN   0.00   0.000   8318.0   \n",
       "6  008411  99999         XM20  NaN   NaN  NaN    NaN     NaN      NaN   \n",
       "7  008414  99999         XM18  NaN   NaN  NaN    NaN     NaN      NaN   \n",
       "8  008415  99999         XM21  NaN   NaN  NaN    NaN     NaN      NaN   \n",
       "9  008418  99999         XM24  NaN   NaN  NaN    NaN     NaN      NaN   \n",
       "\n",
       "       begin        end              geometry  \n",
       "0 2011-03-09 2013-07-30           POINT (0 0)  \n",
       "1 2012-07-13 2017-08-22           POINT (0 0)  \n",
       "2 2014-09-23 2015-09-26           POINT (0 0)  \n",
       "3 2005-01-01 2012-07-31           POINT (0 0)  \n",
       "4 2010-05-19 2012-03-23  POINT (65.567 32.95)  \n",
       "5 2010-04-21 2010-04-21           POINT (0 0)  \n",
       "6 2016-02-17 2016-02-17           POINT EMPTY  \n",
       "7 2016-02-16 2016-02-17           POINT EMPTY  \n",
       "8 2016-02-17 2020-03-14           POINT EMPTY  \n",
       "9 2016-02-17 2016-02-17           POINT EMPTY  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_stations_csv_path = google_drive_personal_key + 'datasets/EFs/weather_data/isd-history.csv'\n",
    "\n",
    "# date formats are specified\n",
    "df_stations_all = pd.read_csv(weather_stations_csv_path, parse_dates=['BEGIN','END'])\n",
    "df_stations_all = gtc_functions.standardise_dfs(df_stations_all)\n",
    "df_stations_all.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1187 weather stations had key information missing so were removed\n"
     ]
    }
   ],
   "source": [
    "# remove stations with key information missing\n",
    "df_stations = df_stations_all.dropna(subset=['lat', 'lon', 'usaf', 'wban'])\n",
    "print(f'{len(df_stations_all)-len(df_stations)} weather stations had key information missing so were removed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/orlandotimmerman/Library/CloudStorage/GoogleDrive-rt582@cam.ac.uk/My Drive/ai4er/python/hurricane/hurricane-harm-herald/notebooks/gtc_functions.py:220: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[concatted_col_name] = df[cols_to_concat].astype(str).apply(\n"
     ]
    }
   ],
   "source": [
    "# generating filename of hourly weather data \n",
    "df_stations = gtc_functions.concat_df_cols(df_stations,'csv_filenames',['usaf','wban'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR FETCHING CLOSEST WEATHER STATIONS TO XBD POINTS\n",
    "\n",
    "def find_fetch_closest_weather_station_data(\n",
    "\tdf_xbd_points: pd.DataFrame,\n",
    "\tdf_noaa: pd.DataFrame,\n",
    "\tdf_stations: pd.DataFrame,\n",
    "\ttime_buffer: Tuple[float,str],\n",
    "\tdownload_dest_dir: str,\n",
    "\tmin_number: int = 1,\n",
    "\tdistance_buffer: float = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"TODO: docstring\n",
    "\tTODO: make this less horrifically janky\n",
    "    N.B. standardise download_dest_dir with other peoples' filepaths\n",
    "    \"\"\"\n",
    "\n",
    "    # pre-assign column of values for assignment\n",
    "    df_xbd_points[['event_start', 'event_end']] = np.nan\n",
    "    df_xbd_points[['closest_stations', 'stations_lat_lons']] = np.nan\n",
    "\n",
    "    # group by event in df_xbd_points\n",
    "    df_xbd_points_grouped = df_xbd_points.groupby('disaster_name')\n",
    "    # for each group in df_xbd_points:\n",
    "    for name,group in df_xbd_points_grouped:\n",
    "\n",
    "        # calculate start and end of event\n",
    "        df_event_weather = df_noaa[df_noaa['name'] == name]\n",
    "        start,end = gtc_functions.calculate_first_last_dates_from_df(group, time_buffer)\n",
    "        # limit stations df to those operational within +/- 1 time_buffer either side of event\n",
    "        df_station_time_lim = df_stations[(df_stations['begin'] <= start) & (df_stations['end'] >= end)]\n",
    "\n",
    "        ignore_csvs = []\n",
    "        # for each xbd observation in group\n",
    "        for index,obs in tqdm(group.iterrows(), total=len(group)):\n",
    "            # limit stations spatially\n",
    "            obs_lat_lons = [obs['lat'], obs['lon']]\n",
    "            df_station_spatial_time_lim = gtc_functions.limit_df_spatial_range(\n",
    "\t\t\t\tdf_station_time_lim, obs_lat_lons, min_number, distance_buffer)\n",
    "\n",
    "            stations_list = []\n",
    "            station_no = 0\n",
    "            while len(stations_list) < min_number:\n",
    "            \n",
    "                # find closest weather station(s) to current weather station (allow closest N, or within limit)\n",
    "                # could make this return ranked, then just iterate through\n",
    "                try:\n",
    "                    station_index = gtc_functions.find_index_closest_point_in_col(\n",
    "\t\t\t\t\t\tgroup['geometry'].loc[index], df_station_spatial_time_lim, 'geometry', which_closest=station_no)\n",
    "                except:\n",
    "                    df_station_spatial_time_lim = gtc_functions.limit_df_spatial_range(\n",
    "\t\t\t\t\t\tdf_station_time_lim, obs_lat_lons, len(df_station_spatial_time_lim)+1)\n",
    "                    station_index = gtc_functions.find_index_closest_point_in_col(\n",
    "\t\t\t\t\t\tgroup['geometry'].loc[index], df_station_spatial_time_lim, 'geometry', which_closest=station_no)\n",
    "\n",
    "                \n",
    "                # TODO: could potentially fail when crossing years, but not realistically with hurricanes\n",
    "                event_year = start.year\n",
    "                # get weather station csv filename\n",
    "                csv_filename = df_station_spatial_time_lim['csv_filenames'].loc[station_index]\n",
    "                url = generate_weather_station_url(event_year, csv_filename)\n",
    "\n",
    "                # executes if weather station not already downloaded\n",
    "                # if file in ignore, reloop to next-closest station\n",
    "                if not '/'.join((str(event_year), csv_filename)) in ignore_csvs:\n",
    "                    # if file doesn't exist, append to ignore and reloop\n",
    "                    # if file not downloaded\n",
    "                    if not check_is_file_downloaded(csv_filename, download_dest_dir):\n",
    "                        try:\n",
    "                            download_dest = download_dest_dir + '.'.join((csv_filename, 'csv'))\n",
    "                            urllib.request.urlretrieve(url, download_dest)\n",
    "                            stations_list.append(csv_filename)\n",
    "                        except:\n",
    "                            ignore_csvs.append('/'.join((str(event_year), csv_filename)))\n",
    "                    else:\n",
    "                        stations_list.append(csv_filename)\n",
    "                station_no += 1\n",
    "\n",
    "            # append list of stations\n",
    "            df_xbd_points['closest_stations'].iloc[index] = stations_list\n",
    "            # append start and end dates\n",
    "            df_xbd_points['event_start'].iloc[index] = start\n",
    "            df_xbd_points['event_end'].iloc[index] = end\n",
    "\n",
    "        # remove station rows which don't exist\n",
    "        df_stations = df_stations.loc[~df_stations['csv_filenames'].isin(ignore_csvs)]\n",
    "\n",
    "    return df_xbd_points\n",
    "\n",
    "\n",
    "def generate_weather_station_url(\n",
    "\tevent_year: str,\n",
    "\tcsv_filename: str\n",
    ") -> str:\n",
    "    url_start = 'https://www.ncei.noaa.gov/data/global-hourly/access/'\n",
    "    return url_start + '/'.join((str(event_year),csv_filename)) + '.csv'\n",
    "\n",
    "\n",
    "def check_does_file_exist(\n",
    "\turl: str\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    False if non-existent, true if exists\n",
    "    \n",
    "    TODO: docstring\"\"\"\n",
    "    # try:\n",
    "    #     urllib.request.urlretrieve(url,download_dest)\n",
    "    #     return True\n",
    "    # except:\n",
    "    #     print(f'{url} does not exist')\n",
    "    #     return False\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def check_is_file_downloaded(\n",
    "\tcsv_filename: str,\n",
    "\tdownload_dest_dir: str\n",
    ") -> bool:\n",
    "    \"\"\"True if already downloaded, False if not\"\"\"\n",
    "    potential_file_path = '/'.join((download_dest_dir,csv_filename)) + '.csv'\n",
    "    if os.path.exists(potential_file_path):\n",
    "        # downloaded\n",
    "        return True\n",
    "    else:\n",
    "\t\tprint(f'{csv_filename} already downloaded.')\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_from_url(\n",
    "\turl: str,\n",
    "\tdownload_dest_dir\n",
    "):\n",
    "    filename = url.split('/')[-1]\n",
    "    destination = '/'.join((download_dest_dir,filename))\n",
    "    # download file\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url,destination)\n",
    "    except:\n",
    "        # for some a few weather station identifiers don't seem to exist\n",
    "\t\tprint(f'{url} could not be found.')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get closest NOAA data for each xbd datapoint\n",
    "\n",
    "def find_NOAA_points(\n",
    "\tdf_noaa_xbd_hurricanes: pd.DataFrame, \n",
    "\tdf_xbd_points: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Append the closest weather data from NOAA 6-hourly data to xbd points\n",
    "\n",
    "\tTODO: DOCSTRING\n",
    "    \"\"\"\n",
    "\n",
    "    noaa_indices = []\n",
    "    xbd_indices = []\n",
    "    distances = []\n",
    "    # group by event in df_xbd_points\n",
    "    df_xbd_points_grouped = df_xbd_points.groupby('disaster_name')\n",
    "    # for each group in df_xbd_points:\n",
    "    for name,group in df_xbd_points_grouped:\n",
    "        df_event_weather = df_noaa_xbd_hurricanes[df_noaa_xbd_hurricanes['name']==name]\n",
    "        \n",
    "        for index,obs in tqdm(group.iterrows(), total=len(group)):\n",
    "\t\t\t# find index of noaa observation datapoint closest to xbd point\n",
    "            noaa_index = gtc_functions.find_index_closest_point_in_col(\n",
    "                group['geometry'].loc[index], df_event_weather, 'geometry')\n",
    "            noaa_row = df_noaa_xbd_hurricanes.loc[noaa_index]\n",
    "\t\t\t# calculate distance between xbd point and noaa observation\n",
    "            distance = geodesic((obs['lat'], obs['lon']), \n",
    "                                (noaa_row['lat'], noaa_row['lon'])).km\n",
    "\n",
    "\t\t\t# append to list as tuple (faster than appending as value)\n",
    "            noaa_indices += noaa_index,\n",
    "            xbd_indices += index,\n",
    "            distances += distance,\n",
    "\n",
    "    # reindex dataframes to prepare for merge\n",
    "    reindexed_noaa_xbd_hurricanes = df_noaa_xbd_hurricanes.reindex(noaa_indices)\n",
    "    reindexed_noaa_xbd_hurricanes = reindexed_noaa_xbd_hurricanes.reset_index().rename(columns={'index': 'noaa_index'})\n",
    "\n",
    "    reindexed_xbd_points = df_xbd_points.reindex(xbd_indices)\n",
    "    reindexed_xbd_points = reindexed_xbd_points.reset_index().rename(columns={'index': 'xbd_index'})\n",
    "\n",
    "    # rename columns before merge to avoid duplicate column names\n",
    "    reindexed_noaa_xbd_hurricanes.rename(\n",
    "\t\tcolumns={'geometry': 'noaa_obs_geometry', 'lon': 'noaa_obs_lon', 'lat': 'noaa_obs_lat','date': 'noaa_obs_date'},\n",
    "\t\tinplace=True)\n",
    "    reindexed_xbd_points.rename(\n",
    "\t\tcolumns={'geometry': 'xbd_obs_geometry', 'lon': 'xbd_obs_lon', 'lat': 'xbd_obs_lat'},\n",
    "\t\tinplace=True)\n",
    "\n",
    "    joined_df = reindexed_xbd_points.join(reindexed_noaa_xbd_hurricanes, how='inner').set_index('xbd_index')\n",
    "    joined_df.sort_values(by='xbd_index', inplace=True)\n",
    "    df = gtc_functions.calc_distance_between_df_cols(\n",
    "\t\tjoined_df, [['noaa_obs_lat', 'noaa_obs_lon'], ['xbd_obs_lat', 'xbd_obs_lon']], 'shortest_distance_to_track')\n",
    "\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gtc_code",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
