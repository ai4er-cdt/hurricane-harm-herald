{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# H3 hurricane model pipeline"
      ],
      "metadata": {
        "id": "fysBfnAPo_dH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5w2YDGr0f7x0"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install rasterio # pip install necessary to avoid error in this package\n",
        "!pip install geopandas\n",
        "!pip install georaster\n",
        "!pip install pandas --upgrade # make sure on latest version of pandas so you can open the pickled files\n",
        "!pip install pytorch_lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "import torchvision.models as models\n",
        "import torchvision\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "import torch\n",
        "import geopandas as gpd\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "from torchvision.io import read_image\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchmetrics\n",
        "from torchmetrics import Accuracy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import torch.nn.functional as F \n",
        "from torchvision.models import ViT_L_16_Weights, vit_l_16\n",
        "from torchvision.models import swin_v2_b, Swin_V2_B_Weights\n",
        "from torchvision.models.swin_transformer import SwinTransformer\n",
        "import random\n",
        "from sklearn.utils.class_weight import compute_class_weight"
      ],
      "metadata": {
        "id": "9au1_kEGf_99"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model pipeline"
      ],
      "metadata": {
        "id": "X-2lxQEIgghf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataAugmentation(nn.Module):\n",
        "    \"\"\"Module to perform data augmentation on torch tensors.\"\"\"\n",
        "    \"\"\"Consider using the augmentations in the below link, as they work on tensors\"\"\"\n",
        "    \"\"\"https://pytorch.org/vision/main/transforms.html\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 use_noise = False,\n",
        "                 use_flipping = True,\n",
        "                 use_rotation = True,\n",
        "                 use_zoom = True,\n",
        "                 use_solarize = False,\n",
        "                 use_colorjitter = False,\n",
        "                 noise_amount = 20, # the std of the noise level, assuming the image pixel value is between 0 to 255 \n",
        "                 flip_probability = 0.5,\n",
        "                 scale_range=(0.9,1.1),\n",
        "                 solarize_threshold=128, # above the solarize_threshold, the pixle will be randomly inverted with probability to 'solarize_probability'\n",
        "                 solarize_probability=0.5,\n",
        "                 cj_brightness=(0.95,1), # colorjitter brightness\n",
        "                 cj_contrast=(0.95,1),  # colorjitter contrast\n",
        "                 cj_saturation=(0.95,1),  # colorjitter saturation\n",
        "                 cj_hue=(-0.15,0.15)) -> None:  ## colorjitter hue\n",
        "        super().__init__()\n",
        "\n",
        "        #the following code apply gaussian noise to the image with the std value of the noise specified by 'noise_amount'\n",
        "        self.apply_noise = transforms.Compose([\n",
        "                            transforms.Lambda(lambda x: x + torch.randn(x.size())*noise_amount),\n",
        "                            transforms.Lambda(lambda x: torch.clamp(x, 0, 255)),\n",
        "                            transforms.Lambda(lambda x: x.int())\n",
        "        ])\n",
        "\n",
        "\n",
        "        self.flipping = nn.Sequential(\n",
        "            transforms.RandomHorizontalFlip(p = flip_probability),\n",
        "            transforms.RandomVerticalFlip(p = flip_probability)\n",
        "        )\n",
        "\n",
        "        self.rotation = transforms.Lambda(lambda x: transforms.functional.rotate(x, random.choice([0, 90, 180, 270])))\n",
        "            \n",
        "\n",
        "        #the following code apply zoom \n",
        "        self.zoom = transforms.RandomAffine(degrees=(0, 0), translate=(0, 0), scale=scale_range)\n",
        "        #the following code apply solarize\n",
        "        #please set the threshold(range:0 to 255), which when exceeded the pixel will be solarized.\n",
        "        #let the threhold to be 0.8 to 1 times of the brightest pixel. \n",
        "        #if set threhold very low (like 0.1) the effect will be like invert, which differs from original a lot.\n",
        "        self.solarize = transforms.RandomSolarize(solarize_threshold,solarize_probability)\n",
        "\n",
        "        # the following code apply colorjitte to the image\n",
        "        self.colorjitter=transforms.Compose([\n",
        "                            transforms.Lambda(lambda x: x/255),\n",
        "                            transforms.ColorJitter(brightness=(0.95,1),contrast=(0.95,1),saturation=(0.95,1),hue=(-0.15,0.15)),\n",
        "                            transforms.Lambda(lambda x: x*255),\n",
        "                            transforms.Lambda(lambda x: x.int())\n",
        "        ])\n",
        "                \n",
        "        self.use_noise = use_noise\n",
        "        self.use_flipping = use_flipping\n",
        "        self.use_rotation = use_rotation\n",
        "        self.use_zoom = use_zoom\n",
        "        self.use_solarize = use_solarize\n",
        "        self.use_colorjitter = use_colorjitter\n",
        "\n",
        "\n",
        "\n",
        "    @torch.no_grad()  # disable gradients for effiency\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.use_flipping:\n",
        "            x = self.flipping(x)\n",
        "\n",
        "        if self.use_noise:\n",
        "            x = self.apply_noise(x)\n",
        "\n",
        "        if self.use_rotation:\n",
        "            x = self.rotation(x)\n",
        "        \n",
        "        if self.use_zoom:\n",
        "            x = self.zoom(x)\n",
        "\n",
        "        if self.use_solarize:\n",
        "            x = self.solarize(x)\n",
        "\n",
        "        if self.use_colorjitter:\n",
        "            x = self.colorjitter(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"initialize the image embedding block\"\"\"\n",
        "\"\"\"in GaLeNet Fig.1 this is the CLIP box\"\"\"\n",
        "class ImageEncoder(pl.LightningModule):\n",
        "    def __init__(self, image_embedding_architecture):\n",
        "        super().__init__()\n",
        "\n",
        "        if image_embedding_architecture == \"ResNet18\":\n",
        "        # tell pytorch to use the ResNet18 architecture\n",
        "\n",
        "            backbone = models.resnet18(weights = \"DEFAULT\")\n",
        "\n",
        "            # drop final layer since non-SSL trained model\n",
        "            # with the below ResNet, num_image_encoder_features == 512\n",
        "            layers = list(backbone.children())[:-1] \n",
        "            self.feature_extractor = nn.Sequential(*layers)\n",
        "\n",
        "        elif image_embedding_architecture == \"ViT_L_16\":\n",
        "            # with the below ViT_L_16, num_image_encoder_features == 1024\n",
        "            backbone = vit_l_16(weights=ViT_L_16_Weights.DEFAULT)\n",
        "            layers = list(backbone.children())[:-1] \n",
        "            self.feature_extractor = nn.Sequential(*layers)\n",
        "            self.model = backbone\n",
        "\n",
        "        elif image_embedding_architecture == \"Swin_V2_B\":\n",
        "            # with the below Swin_V2_B, num_image_encoder_features == 1024\n",
        "            backbone = swin_v2_b(weights=Swin_V2_B_Weights.DEFAULT)\n",
        "            layers = list(backbone.children())[:-1] \n",
        "            self.feature_extractor = nn.Sequential(*layers)\n",
        "\n",
        "        #elif image_embedding_architecture == \"SatMAE\":\n",
        "            # with the below SatMAE, num_image_encoder_features == 1024\n",
        "        #    self.model = h3.models.vision_transformer.get_model(\"SatMAE\")\n",
        "\n",
        "\n",
        "        self.image_embedding_architecture = image_embedding_architecture\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.feature_extractor.eval()\n",
        "\n",
        "        with torch.no_grad(): \n",
        "            if self.image_embedding_architecture == \"ResNet18\":\n",
        "                embedding = self.feature_extractor(x).flatten(1)\n",
        "\n",
        "            elif self.image_embedding_architecture == \"ViT_L_16\":\n",
        "                # the following code is taken from https://discuss.pytorch.org/t/feature-extraction-in-torchvision-models-vit-b-16/148029/2\n",
        "\n",
        "                # This is the whole encoder sequence\n",
        "                encoder = self.feature_extractor[1]\n",
        "\n",
        "                # This is how the model preprocess the image.\n",
        "                # The output shape is the one desired \n",
        "                x = self.model._process_input(x)\n",
        "\n",
        "                n = x.shape[0]\n",
        "\n",
        "                batch_class_token = self.model.class_token.expand(n, -1, -1)\n",
        "                x = torch.cat([batch_class_token, x], dim = 1)\n",
        "                x = encoder(x)\n",
        "\n",
        "                # Classifier \"token\" as used by standard language architectures\n",
        "                embedding = x[:, 0]\n",
        "\n",
        "            elif self.image_embedding_architecture == \"Swin_V2_B\":\n",
        "                embedding = self.feature_extractor(x)\n",
        "\n",
        "            #elif SatMAE\n",
        "\n",
        "        return embedding\n",
        "\n",
        "\n",
        "\"\"\"initalize the generic encoder block\"\"\"\n",
        "\"\"\"in GaLeNet Fig.1 this is all of the small grey Encoder Blocks\"\"\"\n",
        "class GenericEncoder(pl.LightningModule):\n",
        "    def __init__(self, num_input_features, num_output_features, dropout_rate): \n",
        "        super().__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(num_input_features, num_output_features)\n",
        "        self.batchnorm = nn.BatchNorm1d(num_output_features)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.activation = nn.SiLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"initalize the SoftMax classification layer to predict damage class\"\"\"\n",
        "class ClassificationLayer(pl.LightningModule):\n",
        "    def __init__(self, num_input_features, num_output_classes, output_activation): \n",
        "        super().__init__()\n",
        "\n",
        "        \n",
        "        if output_activation == \"sigmoid\": # use Sigmoid for binary classification\n",
        "            self.activation = nn.Sigmoid()\n",
        "            self.l1 = nn.Linear(num_input_features, 1)\n",
        "\n",
        "        elif output_activation == \"softmax\": #softmax for multiclass classification\n",
        "            self.activation = torch.nn.Softmax(dim = 1)\n",
        "            self.l1 = self.l1 = nn.Linear(num_input_features, num_output_classes)\n",
        "\n",
        "        elif output_activation == \"relu\":\n",
        "            \"\"\"relu could be used if we treat damage classes as a regression\n",
        "            problem. this is unbounded though so can produce values > 4.\"\"\"\n",
        "            self.activation = F.relu\n",
        "            self.l1 = self.l1 = nn.Linear(num_input_features, 1)\n",
        "\n",
        "        elif output_activation == None:\n",
        "            self.activation = nn.Identity()\n",
        "            self.l1 = self.l1 = nn.Linear(num_input_features, num_output_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.l1(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\"\"\"initalize the overall architecture, i.e. the combination of encoder blocks\"\"\"\n",
        "class OverallModel(pl.LightningModule):\n",
        "    \"\"\"\n",
        "    Description of what this class does here\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    training_dataset : torch.utils.data.Dataset\n",
        "        Contains the data used for training\n",
        "\n",
        "    validation_dataset : torch.utils.data.Dataset\n",
        "        Contains the data used for training\n",
        "\n",
        "    image_embedding_architecture : str\n",
        "        Determines the image embedding architecture used. Possible values are:\n",
        "            - 'ResNet18'\n",
        "            - 'ViT_L_16'\n",
        "            - 'Swin_V2_B'\n",
        "\n",
        "    num_input_channels : int\n",
        "        The number of channels in the input images.\n",
        "\n",
        "    dropout_rate : float\n",
        "        The dropout probability\n",
        "\n",
        "    image_encoder_lr : float\n",
        "        The learning rate for the image encoder. If 0, then image encoder weights are frozen.\n",
        "\n",
        "    general_lr : float\n",
        "        The learning rate for all other parts of the model.\n",
        "\n",
        "    batch_size : int\n",
        "        The batch size used during training and validation steps.\n",
        "\n",
        "    weight_decay : float\n",
        "        Adam weight decay (L2 penalty)\n",
        "\n",
        "    lr_scheduler_patience : int\n",
        "        The number of epochs of validation loss plateau before lr is decreased.\n",
        "\n",
        "    num_image_feature_encoder_features : int\n",
        "        The number of features output from the encoder that operates on the \n",
        "        features produced by the image encoder\n",
        "\n",
        "    num_output_classes : int\n",
        "        The number of output classes. Set to 1 for regression.\n",
        "\n",
        "    zoom_levels : List[str]\n",
        "        A list containing the different image zoom levels.\n",
        "\n",
        "    class_weights: torch.FloatTensor\n",
        "        A tensor containing a weights to be applied to each class in the \n",
        "        cross entropy loss function.\n",
        "\n",
        "    image_only_model: Boolean\n",
        "        If true, then the model behaves as if there were no EFs, and only the\n",
        "        images are used to make predictions.\n",
        "\n",
        "    loss_function_str : str\n",
        "        Determines the loss function used. Possible values are:\n",
        "            - 'BCELoss' : Binary Cross Entropy Loss, for binary classification\n",
        "            - 'CELoss' : Cross Entropy Loss, for multiclass classification\n",
        "            - 'MSE' : Mean Squared Error, for regression\n",
        "\n",
        "    output_activation : str\n",
        "        Determines the output activation function used. Possible values are:\n",
        "            - 'sigmoid' : Sigmoid, for binary classification\n",
        "            - 'softmax' : Softmax, for multiclass classification\n",
        "            - 'relu' : ReLU, for regression\n",
        "\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    Describe the attributes here, e.g. image_encoder, classification, augment\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        training_dataset, \n",
        "        validation_dataset,\n",
        "        image_embedding_architecture = \"ResNet18\",\n",
        "        dropout_rate: float = 0.2,\n",
        "        general_lr: float = 1e-4,\n",
        "        image_encoder_lr: float = 0,\n",
        "        batch_size = 32,\n",
        "        weight_decay = 0,\n",
        "        lr_scheduler_patience = 2, \n",
        "        num_input_channels = 3,\n",
        "        num_EFs = 15,\n",
        "        #num_EF_embedding_features = 20,\n",
        "        num_concat_encoder_features = 100,\n",
        "        num_image_feature_encoder_features = 56,\n",
        "        num_output_classes = 4,\n",
        "        zoom_levels = [\"1\"],\n",
        "        class_weights = None,\n",
        "        image_only_model = False,\n",
        "        \n",
        "        loss_function_str = \"CELoss\", # maybe use focal loss for unbalanced multiclass as in GaLeNet\n",
        "        output_activation = None # CELoss expects unnormalized logits\n",
        "\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        if image_embedding_architecture == \"ResNet18\":\n",
        "            num_image_encoder_features = 512\n",
        "        else: # every other case should be a ViT which outputs 1024 features\n",
        "            num_image_encoder_features = 1024\n",
        "\n",
        "        # the following line come from the GaLeNet paper\n",
        "        num_EF_embedding_features = num_EFs\n",
        "\n",
        "        self.augment = DataAugmentation()\n",
        "\n",
        "        # the image encoding architecture (e.g. ResNet)\n",
        "        self.image_encoder = ImageEncoder(\n",
        "            image_embedding_architecture\n",
        "        )\n",
        "\n",
        "        # need nn.ModuleList() to create a variable number of encoders depending\n",
        "        # on the zoom levels supplied\n",
        "        self.image_feature_encoders = nn.ModuleList()\n",
        "        for _ in zoom_levels:\n",
        "            # the encoding block for image features (produces Ai1 as in the diagram)\n",
        "            self.image_feature_encoders.append(GenericEncoder(\n",
        "                num_image_encoder_features, num_image_feature_encoder_features, dropout_rate\n",
        "            ))\n",
        "\n",
        "        self.image_feature_classifiers = nn.ModuleList()\n",
        "        for _ in zoom_levels:\n",
        "            # the classification block for each embedded zoomed image\n",
        "            self.image_feature_classifiers.append(ClassificationLayer(\n",
        "                num_image_feature_encoder_features, num_output_classes, output_activation\n",
        "            ))\n",
        "\n",
        "        if not image_only_model:\n",
        "            # an encoding block for all EFs. for now, we don't put different EFs into different encoders.\n",
        "            self.ef_encoder = GenericEncoder(\n",
        "                num_EFs, num_EF_embedding_features, dropout_rate\n",
        "            )\n",
        "\n",
        "        if not image_only_model:\n",
        "            # the encoder that takes as input the encoded image features + encoded EFs\n",
        "            self.concat_encoder = GenericEncoder(\n",
        "                (num_image_feature_encoder_features * len(zoom_levels)) + num_EF_embedding_features, num_concat_encoder_features, dropout_rate  \n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # the encoder that takes as input the encoded image features\n",
        "            self.concat_encoder = GenericEncoder(\n",
        "                (num_image_feature_encoder_features * len(zoom_levels)), num_concat_encoder_features, dropout_rate  \n",
        "            )\n",
        "\n",
        "        # the classification layer used with the concatenated embedded features\n",
        "        self.concat_classification = ClassificationLayer(num_concat_encoder_features, num_output_classes, output_activation)\n",
        "\n",
        "\n",
        "        # \"\"\"below, num_input_features should be some parameter that contains the number of weather related features\"\"\"\n",
        "        # self.weather_encoder = GenericEncoder(\n",
        "        #     num_input_features, num_output_features, dropout_rate  \n",
        "        # )\n",
        "\n",
        "        # \"\"\"below, num_input_features should be some parameter that contains the number of DEM related features\"\"\"\n",
        "        # self.dem_encoder = GenericEncoder(\n",
        "        #     num_input_features, num_output_features, dropout_rate  \n",
        "        # )\n",
        "\n",
        "        # \"\"\"there will be one storm surge feature. how, if at all, should this be encoded?\"\"\"\n",
        "        # self.storm_surge_encoder = GenericEncoder(\n",
        "        #     num_input_features, num_output_features, dropout_rate  \n",
        "        # )\n",
        "\n",
        "        # \"\"\"... more EF encoders\"\"\"\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        if image_encoder_lr == 0:\n",
        "            self.image_encoder.freeze()\n",
        "\n",
        "        if loss_function_str == \"BCELoss\":\n",
        "            self.loss_function = torch.nn.BCELoss()\n",
        "\n",
        "        elif loss_function_str == \"CELoss\":\n",
        "            self.loss_function = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        elif loss_function_str == \"MSE\":\n",
        "            self.loss_function = torch.nn.MSELoss()     \n",
        "\n",
        "        elif loss_function_str == \"weighted_CELoss\":\n",
        "            self.loss_function = torch.nn.CrossEntropyLoss(weight = class_weights)\n",
        "\n",
        "\n",
        "        self.image_encoder_lr = image_encoder_lr\n",
        "        self.general_lr = general_lr\n",
        "        self.batch_size = batch_size\n",
        "        self.lr_scheduler_patience = lr_scheduler_patience\n",
        "        self.zoom_levels = zoom_levels\n",
        "        self.weight_decay = weight_decay\n",
        "        self.image_only_model = image_only_model\n",
        "\n",
        "        self.training_dataset = training_dataset\n",
        "        self.validation_dataset = validation_dataset\n",
        "\n",
        "        # balanced accuracy (i think)\n",
        "        self.accuracy = Accuracy(task = 'multiclass', average = 'macro', num_classes = num_output_classes)\n",
        "\n",
        "\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"for each zoom level Z, do image_Z_embedding = self.image_encoder(inputs[\"image_Z\"], image_embedding_architecture)\"\"\"\n",
        "\n",
        "        \"\"\"for each zoom level Z, do image_Z_embedding = self.GenericEncoder(image_Z_embedding, num_input_features, num_output_features)\"\"\"\n",
        "\n",
        "\n",
        "        \"\"\"for each type of EF, do EF_embedding = self.image_encoder(inputs[\"EF\"])\"\"\"\n",
        "        \"\"\"all related EFs (e.g. all the weather EFs) should be in a\n",
        "        single vector and pushed through a single embedding block.\"\"\"\n",
        "        \"\"\"there should be a different embedding block for each type of EF\"\"\"\n",
        "\n",
        "        \"\"\"concat_embedding = concat all EF_embeddings and all image_Z_embeddings\"\"\"\n",
        "        \"\"\"concat_embedding = GenericEncoder(concat_embedding)\"\"\"\n",
        "        \"\"\"concat_prediction = self.ClassificationLayer(concat_prediction)\"\"\"\n",
        "\n",
        "        \"\"\"for each zoom level Z, do Z_prediction = self.ClassificationLayer(image_Z_embedding)\"\"\"\n",
        "\n",
        "\n",
        "        \"\"\"return the predictions from each zoom level individually and also the \n",
        "        predciction from the concat_embedding\"\"\"\n",
        "\n",
        "        # return Z1_prediction, Z2_prediction, ..., concat_prediction\n",
        "\n",
        "        # a list of tensors to be concatenated\n",
        "        embeddings_to_concat = []\n",
        "\n",
        "        # for each zoom level, put the image embedding tensor into the list\n",
        "        for i in range(len(self.zoom_levels)):\n",
        "            zoom_level = self.zoom_levels[i]\n",
        "            image_zoom_embedding = self.image_encoder(inputs[\"img_zoom_\" + zoom_level])\n",
        "            image_zoom_embedding = self.image_feature_encoders[i](image_zoom_embedding)\n",
        "            embeddings_to_concat.append(image_zoom_embedding)\n",
        "\n",
        "\n",
        "        # a list of the predictions made from each embedded zoom level\n",
        "        image_feature_predictions = []\n",
        "\n",
        "        # for each embedded zoom level, predict the output class\n",
        "        for i in range(len(embeddings_to_concat)):\n",
        "            image_feature_predictions.append(self.image_feature_classifiers[i](embeddings_to_concat[i]))\n",
        "\n",
        "        if not self.image_only_model:\n",
        "            # put the embedded EFs into the the list\n",
        "            embeddings_to_concat.append(self.ef_encoder(inputs[\"EFs\"]))\n",
        "\n",
        "        # concats the EF and zoomed image embeddings. first dim is batch dimension, so concat along dim = 1\n",
        "        concat_embedding = torch.concat(embeddings_to_concat, dim = 1)\n",
        "        concat_embedding = self.concat_encoder(concat_embedding)\n",
        "\n",
        "        concat_predictions = self.concat_classification(concat_embedding)\n",
        "\n",
        "        return concat_predictions, image_feature_predictions\n",
        "\n",
        "\n",
        "    def _compute_losses(self, concat_predictions, image_feature_predictions, y):\n",
        "    #def _compute_losses(self, Z1_prediction, Z2_prediction, ..., concat_prediction, y):\n",
        "\n",
        "        \"\"\"Z1_loss = focal_loss(Z1_prediction, y)\"\"\"\n",
        "        \"\"\"Z2_loss = focal_loss(Z2_prediction, y)\"\"\"\n",
        "        \"\"\"...\"\"\"\n",
        "        \"\"\"concat_loss = focal_loss(concat_loss, y)\"\"\"\n",
        "\n",
        "        \"\"\"loss = sum(Z1_loss to Z4_loss) + concat_loss\"\"\"\n",
        "        \"\"\"each of the individual loss functions is a torchvision.ops.focal_loss, as in GaLeNet\"\"\"\n",
        "        \"\"\"the L_i's correspond to different zoom levels, so only use one L_i to start with\"\"\"\n",
        "\n",
        "        #display(predictions.shape)\n",
        "        #display(y.shape)\n",
        "        #display(y.flatten().shape)\n",
        "\n",
        "        loss = self.loss_function(concat_predictions, y.flatten())\n",
        "\n",
        "        # as in GaLeNet, combine losses from concat embedding and each of the \n",
        "        # zoomed image embeddings\n",
        "        for image_feature_prediction in image_feature_predictions:\n",
        "            loss += self.loss_function(image_feature_prediction, y.flatten())\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "\n",
        "        # this code allows different learning rates for the different blocks\n",
        "        if not self.image_only_model:\n",
        "            parameters = [\n",
        "                {\"params\": self.image_encoder.parameters(), \"lr\": self.image_encoder_lr, \"weight_decay\": self.weight_decay},\n",
        "                {\"params\": self.concat_classification.parameters(), \"lr\": self.general_lr, \"weight_decay\": self.weight_decay},\n",
        "                {\"params\": self.concat_encoder.parameters(), \"lr\": self.general_lr, \"weight_decay\": self.weight_decay},\n",
        "                {\"params\": self.ef_encoder.parameters(), \"lr\": self.general_lr, \"weight_decay\": self.weight_decay},\n",
        "                {\"params\": self.image_feature_classifiers.parameters(), \"lr\": self.general_lr, \"weight_decay\": self.weight_decay},\n",
        "                {\"params\": self.image_feature_encoders.parameters(), \"lr\": self.general_lr, \"weight_decay\": self.weight_decay}\n",
        "            ]\n",
        "        else:\n",
        "            # this else just removes ef_encoder\n",
        "            parameters = [\n",
        "                {\"params\": self.image_encoder.parameters(), \"lr\": self.image_encoder_lr, \"weight_decay\": self.weight_decay},\n",
        "                {\"params\": self.concat_classification.parameters(), \"lr\": self.general_lr, \"weight_decay\": self.weight_decay},\n",
        "                {\"params\": self.concat_encoder.parameters(), \"lr\": self.general_lr, \"weight_decay\": self.weight_decay},\n",
        "                {\"params\": self.image_feature_classifiers.parameters(), \"lr\": self.general_lr, \"weight_decay\": self.weight_decay},\n",
        "                {\"params\": self.image_feature_encoders.parameters(), \"lr\": self.general_lr, \"weight_decay\": self.weight_decay}\n",
        "            ]\n",
        "\n",
        "        optimizer = torch.optim.Adam(parameters)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode = \"min\",\n",
        "            patience = self.lr_scheduler_patience\n",
        "        )\n",
        "        return {\n",
        "            \"optimizer\": optimizer,\n",
        "            \"lr_scheduler\": lr_scheduler,\n",
        "            \"monitor\": \"val/loss\",\n",
        "        }\n",
        "\n",
        "\n",
        "    def training_step(self, batch, *args, **kwargs):\n",
        "        x, y = batch\n",
        "\n",
        "        concat_predictions, image_feature_predictions = self.forward(x)\n",
        "        loss = self._compute_losses(concat_predictions, image_feature_predictions, y).mean() \n",
        "\n",
        "        acc = self.accuracy(concat_predictions, y)\n",
        "\n",
        "        # belwo is for multizoom loss\n",
        "        #Z1_prediction, Z2_prediction, ..., concat_prediction = self.forward(x)\n",
        "        #loss = self._compute_losses(Z1_prediction, Z2_prediction, ..., concat_prediction, y).mean() # maybe normalize the loss?\n",
        "\n",
        "        train_loss = self.all_gather(loss) # what does all_gather do?\n",
        "        self.log(\"train/loss\", train_loss.mean(), logger = True, on_epoch = True)\n",
        "        self.log(\"train accuracy\", acc, logger = True, on_epoch = True)\n",
        "\n",
        "        return train_loss\n",
        "\n",
        "    def validation_step(self, batch, *args, **kwargs):\n",
        "        x, y = batch\n",
        "\n",
        "        concat_predictions, image_feature_predictions = self.forward(x)\n",
        "        loss = self._compute_losses(concat_predictions, image_feature_predictions, y).mean() \n",
        "\n",
        "        acc = self.accuracy(concat_predictions, y)\n",
        "\n",
        "        # below code is for multi-zoom processing\n",
        "        #Z05_prediction, Z1_prediction, Z2_prediction, ..., concat_prediction = self.forward(x)\n",
        "        #loss = self._compute_losses(Z1_prediction, Z2_prediction, ..., concat_prediction, y).mean()\n",
        "\n",
        "        val_loss = self.all_gather(loss) # what does all_gather do?\n",
        "        self.log(\"val/loss\", val_loss.mean(), logger = True, on_epoch = True)\n",
        "        self.log(\"val accuracy\", acc, logger = True, on_epoch = True)\n",
        "        return val_loss\n",
        "\n",
        "    def training_epoch_end(self, out):\n",
        "        # put stuff here that happens at the end of every training epoch\n",
        "        #self.log('train_acc_epoch', self.accuracy)\n",
        "        pass\n",
        "\n",
        "    def validation_epoch_end(self, out):\n",
        "        # put stuff here that happens at the end of every validation epoch\n",
        "        #self.log('validation_acc_epoch', self.accuracy)\n",
        "        pass\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        loader = DataLoader(self.training_dataset, batch_size = self.batch_size)\n",
        "        return loader\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        loader = DataLoader(self.validation_dataset, batch_size = self.batch_size)\n",
        "        return loader"
      ],
      "metadata": {
        "id": "bQcxctaKgPp1"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image processing"
      ],
      "metadata": {
        "id": "qs3FbZXwhUhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HurricaneDataset(Dataset):\n",
        "    def __init__(self, dataframe, img_path, EF_features, image_embedding_architecture, augmentations = None, zoom_levels = [\"1\"]):\n",
        "        self.dataframe = dataframe\n",
        "        self.img_path = img_path\n",
        "        self.EF_features = EF_features\n",
        "        self.zoom_levels = zoom_levels\n",
        "\n",
        "        if image_embedding_architecture == \"ResNet18\":\n",
        "            self.preprocessing = transforms.Compose([\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "                ])\n",
        "            \n",
        "        elif image_embedding_architecture == \"ViT_L_16\":\n",
        "            self.preprocessing = ViT_L_16_Weights.IMAGENET1K_V1.transforms()\n",
        "        \n",
        "        elif image_embedding_architecture == \"Swin_V2_B\":\n",
        "            self.preprocessing = Swin_V2_B_Weights.IMAGENET1K_V1.transforms()\n",
        "\n",
        "        elif image_embedding_architecture == \"SatMAE\":\n",
        "            # values from CustomDatasetFromImages() in https://github.com/sustainlab-group/SatMAE/blob/main/util/datasets.py\n",
        "            self.preprocessing = transforms.Compose([\n",
        "                    transforms.CenterCrop(224),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize(mean=[0.4182007312774658, 0.4214799106121063, 0.3991275727748871], \n",
        "                                         std=[0.28774282336235046, 0.27541765570640564, 0.2764017581939697]),\n",
        "                ])\n",
        "\n",
        "        if augmentations is not None:\n",
        "            self.transform = transforms.Compose([augmentations, self.preprocessing])\n",
        "\n",
        "        else:\n",
        "            self.transform = self.preprocessing\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.dataframe[\"id\"].iloc[idx]\n",
        "\n",
        "        zoomed_images = {}\n",
        "        for zoom_level in self.zoom_levels:\n",
        "            path = os.path.join(self.img_path, \"zoom_\" + zoom_level, str(image_id) + \".png\")\n",
        "\n",
        "            img = Image.open(os.path.join(self.img_path, \"zoom_\" + zoom_level, str(image_id) + \".png\"))\n",
        "            img = self.transform(img)\n",
        "            img = np.asarray(img)\n",
        "            #img = np.swapaxes(img, 0, 2)\n",
        "\n",
        "            zoomed_images[\"img_zoom_\" + zoom_level] = np.copy(img)\n",
        "\n",
        "        #idx_EFs = [int(self.dataframe[ef].iloc[idx]) for ef in EF_features]\n",
        "\n",
        "        idx_EFs = torch.as_tensor(self.dataframe[self.EF_features].iloc[idx]).type(torch.FloatTensor)\n",
        "\n",
        "        # from risk df\n",
        "        # storm_surge_ef = self.dataframe[\"max_sust_wind\"].iloc[idx]\n",
        "        \n",
        "        label = torch.as_tensor(self.dataframe[\"damage_class\"].iloc[idx]).type(torch.LongTensor)\n",
        "        \n",
        "        # add Weather EFs\n",
        "\n",
        "        # put it in a dictionary so don't have to return a tonne of different values\n",
        "        # img also goes in the below dictionary\n",
        "        # x = {\"storm_surge_ef\": storm_surge_ef, \"soil_ef\": soil_ef}\n",
        "\n",
        "\n",
        "        # EFs = concat all EFs\n",
        "        # 0-1 normalize all EFs\n",
        "        # mean, std\n",
        "\n",
        "        x = {\"EFs\": idx_EFs}\n",
        "        x.update(zoomed_images)\n",
        "\n",
        "        #print(\"x=\",x)\n",
        "        \n",
        "        \n",
        "        # torch.nn.CrossEntropyLoss expects integer labels, not one-hot labels\n",
        "        # see https://stackoverflow.com/questions/62456558/is-one-hot-encoding-required-for-using-pytorchs-cross-entropy-loss-function\n",
        "        # label = F.one_hot(label, num_classes = 5)\n",
        "        \n",
        "\n",
        "        return x, label"
      ],
      "metadata": {
        "id": "QPQFcQOihV2N"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load datasets"
      ],
      "metadata": {
        "id": "En31A7bagV-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Call function from basic models ipynb\n",
        "from typing import List, Union\n",
        "from pathlib import Path\n",
        "from functools import reduce\n",
        "\n",
        "def check_files_in_list_exist(\n",
        "    file_list: Union[List[str], List[Path]]\n",
        "    ):\n",
        "    \"\"\"State which files don't exist and remove from list\"\"\"\n",
        "    files_found = []\n",
        "    for fl in file_list:\n",
        "        # attempt conversion to Path object if necessary\n",
        "        if type(fl) != Path:\n",
        "            try:\n",
        "                fl = Path(fl)\n",
        "            except TypeError:\n",
        "                print(f'{fl} could not be converted to Path object')\n",
        "        \n",
        "        if fl.is_file():\n",
        "            files_found += fl,\n",
        "        else:\n",
        "            print(f'{fl} not found. Removing from list.')\n",
        "\n",
        "    return files_found\n",
        "\n",
        "\n",
        "def read_and_merge_pkls(\n",
        "    pkl_paths: Union[List[str], List[Path]]\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Read in pkl files from list of file paths and merge on index\"\"\"\n",
        "    # check all files exist\n",
        "    pkl_paths_present = check_files_in_list_exist(pkl_paths)\n",
        "    df_list = [pd.read_pickle(pkl) for pkl in pkl_paths_present]\n",
        "\n",
        "    return reduce(lambda df1,df2: pd.merge(df1,df2,left_index=True,right_index=True), df_list)\n",
        "\n",
        "\n",
        "def rename_and_drop_duplicated_cols(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Drop columns which are copies of others and rename the 'asdf_x' headers which would have resulted\"\"\"\n",
        "    # need to ensure no bad types first\n",
        "    df = drop_cols_containing_lists(df)\n",
        "    # remove duplicated columns\n",
        "    dropped_df = df.T.drop_duplicates().T\n",
        "    # rename columns for clarity (especially those which are shared between dfs). Will be able to remove most with better\n",
        "    # column naming further up the process\n",
        "    new_col_names = {col: col.replace('_x', '') for col in dropped_df.columns if col.endswith('_x')}\n",
        "    \n",
        "    return dropped_df.rename(columns=new_col_names)\n",
        "\n",
        "def drop_cols_containing_lists(\n",
        "    df: pd.DataFrame\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"It seemed like the best solution at the time: and to be fair, I can't really think of better...\n",
        "    N.B. for speed, only looks at values in first row – if there is a multi-type column, this would be the least of\n",
        "    our worries...\n",
        "    \"\"\"\n",
        "    df = df.loc[:, df.iloc[0].apply(lambda x: type(x) != list)]\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "sSRXdOmgqNBW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount(\"/content/drive/\")\n",
        "data_dir = \"/content/drive/MyDrive/ai4er/python/hurricane/hurricane-harm-herald/data/xBD_data/\"\n",
        "#xbd_dir = \"/content/drive/MyDrive/ai4er/python/hurricane/hurricane-harm-herald/data/datasets/xBD_data\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIOL6Yh0giwN",
        "outputId": "4c0d45a9-8edc-4b66-880d-11690e004d9f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_noaa_xbd_pkl_path = os.path.join(data_dir + 'EFs/weather_data/xbd_obs_noaa_six_hourly.pkl')\n",
        "# terrain efs\n",
        "#df_terrain_efs_path = os.path.join(data_dir  + \"processed_data/Terrian_EFs.pkl\")\n",
        "# flood, storm surge and soil properties\n",
        "df_topographic_efs_path = os.path.join(data_dir,\"processed_data/df_points_posthurr_flood_risk_storm_surge_soil_properties.pkl\")"
      ],
      "metadata": {
        "id": "dmv1JgR4plZO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pkl_paths = [df_noaa_xbd_pkl_path, df_topographic_efs_path, df_terrain_efs_path]\n",
        "pkl_paths = [df_noaa_xbd_pkl_path, df_topographic_efs_path]\n",
        "EF_df = read_and_merge_pkls(pkl_paths)"
      ],
      "metadata": {
        "id": "9o10ilYrprpN"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EF_df_no_dups = rename_and_drop_duplicated_cols(EF_df)"
      ],
      "metadata": {
        "id": "YtdAEHiIqsGm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EF_df_no_dups.drop(df.columns[1], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "gmq-hu7dd6_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EF_df_no_dups.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYNjeWbQe2qP",
        "outputId": "c5a4ac1e-3e45-4313-cb70-4de77ade5b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['xbd_obs_geometry', 'damage_class', 'disaster_name', 'capture_date',\n",
              "       'xbd_obs_lon', 'xbd_obs_lat', 'event_start', 'event_end',\n",
              "       'stations_lat_lons', 'noaa_index', 'tag', 'num_entries',\n",
              "       'noaa_obs_date', 'record_id', 'sys_status', 'noaa_obs_lat',\n",
              "       'noaa_obs_lon', 'max_sust_wind', 'min_p', 'r_ne_34', 'r_se_34',\n",
              "       'r_nw_34', 'r_sw_34', 'r_ne_50', 'r_se_50', 'r_nw_50', 'r_sw_50',\n",
              "       'r_ne_64', 'r_se_64', 'r_nw_64', 'r_sw_64', 'strength',\n",
              "       'noaa_obs_geometry', 'shortest_distance_to_track', 'disaster_name_y',\n",
              "       'flood_risk', 'storm_surge', 'soil_density', 'sand_content',\n",
              "       'clay_content', 'silt_content'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download manually zipped images locally, then unzip"
      ],
      "metadata": {
        "id": "bGRI9-LY1KWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/drive/MyDrive/images-001.zip /content\n",
        "!cp -r /content/drive/MyDrive/images-002.zip /content"
      ],
      "metadata": {
        "id": "u31tXsHL-a5C"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q -o images-001.zip -d /content\n",
        "!unzip -q -o images-002.zip -d /content"
      ],
      "metadata": {
        "id": "db6dyg6Gg9LW"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the below directory should be to the .pkl with all EFs\n",
        "img_path = \"/content/images/\"\n",
        "\n",
        "EF_df_no_dups[\"id\"] = EF_df_no_dups.index\n",
        "df = EF_df_no_dups.head(100)"
      ],
      "metadata": {
        "id": "jdHUNu7PguX7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model loading"
      ],
      "metadata": {
        "id": "0MGpCPmUhaV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " EF_features_old = [\"max_sust_wind\", \"soil_density\",\n",
        "                       \"sand_content\", \"clay_content\", \"silt_content\",\n",
        "                       \"elevation\", \"aspect\", \"dis2coast\",\n",
        "                       \"shortest_distance_to_track\", \"max_sust_wind\", \"min_p\", \n",
        "                       \"r_ne_34\", \"r_se_34\", \"r_nw_34\", \"r_sw_34\", \"r_ne_50\",\n",
        "                       \"r_se_50\", \"r_nw_50\", \"r_sw_50\", \"r_ne_64\", \"r_se_64\",\n",
        "                       \"r_nw_64\", \"r_sw_64\", \"strength\"]"
      ],
      "metadata": {
        "id": "bxgtFkA7fgCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " EF_features = [\"max_sust_wind\", \"soil_density\",\n",
        "                       \"sand_content\", \"clay_content\", \"silt_content\",\n",
        "                       \"shortest_distance_to_track\", \"max_sust_wind\", \"min_p\", \n",
        "                       \"r_ne_34\", \"r_se_34\", \"r_nw_34\", \"r_sw_34\", \"r_ne_50\",\n",
        "                       \"r_se_50\", \"r_nw_50\", \"r_sw_50\", \"r_ne_64\", \"r_se_64\",\n",
        "                       \"r_nw_64\", \"r_sw_64\", \"strength\"]"
      ],
      "metadata": {
        "id": "rXoXnj_Ufg4U"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, val_df = train_test_split(df, test_size = 0.2)"
      ],
      "metadata": {
        "id": "iPQVuPxShZjc"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#augmentations = DataAugmentation()\n",
        "augmentations = None\n",
        "\n",
        "trainset = HurricaneDataset(train_df, img_path, EF_features,\n",
        "                            image_embedding_architecture = \"ResNet18\",\n",
        "                            zoom_levels = [\"1\", \"2\", \"4\", \"0.5\"],\n",
        "                            augmentations = augmentations)"
      ],
      "metadata": {
        "id": "NONq5NkYhieR"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "rows, cols = 2, 2\n",
        "    plt.subplot(rows, cols, 1)\n",
        "    plt.imshow(torch.squeeze(x['img_zoom_0.5'], dim = 0))\n",
        "    plt.subplot(rows, cols, 2)\n",
        "    plt.imshow(torch.squeeze(x['img_zoom_1'], dim = 0))\n",
        "    plt.subplot(rows, cols, 3)\n",
        "    plt.imshow(torch.squeeze(x['img_zoom_2'], dim = 0))\n",
        "    plt.subplot(rows, cols, 4)\n",
        "    plt.imshow(torch.squeeze(x['img_zoom_4'], dim = 0))\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "orWMKh8QYMOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a little code here to see what the dataloader actually produces\n",
        "loader = torch.utils.data.DataLoader(trainset, batch_size=8,\n",
        "                                         shuffle=False, num_workers=1)\n",
        "\n",
        "for batch in loader:\n",
        "    #storm_surge_ef, soil_ef, label = batch\n",
        "    x, y = batch\n",
        "    print(y.shape)\n",
        "    print(y)\n",
        "    #print(x['img_zoom_1'] - x['img_zoom_2'])\n",
        "    break"
      ],
      "metadata": {
        "id": "DrPVQRd7hkQm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47249d7-1ae0-449e-90f5-2cdea9bc2509"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8])\n",
            "tensor([1, 4, 2, 2, 0, 2, 0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "zoom_levels = [\"1\", \"2\", \"4\", \"0.5\"]\n",
        "\n",
        "# class weights for weighted cross-entropy loss\n",
        "class_weights = compute_class_weight(class_weight = \"balanced\",\n",
        "                                     classes = np.unique(train_df[\"damage_class\"].to_numpy()),\n",
        "                                     y = train_df[\"damage_class\"])\n",
        "\n",
        "class_weights = torch.as_tensor(class_weights).type(torch.FloatTensor)\n",
        "\n",
        "\n",
        "train_dataset = HurricaneDataset(train_df, img_path, EF_features, image_embedding_architecture = \"ResNet18\", zoom_levels = zoom_levels)\n",
        "val_dataset = HurricaneDataset(val_df, img_path, EF_features, image_embedding_architecture = \"ResNet18\", zoom_levels = zoom_levels)\n",
        "\n",
        "model = OverallModel(training_dataset = train_dataset, \n",
        "                     validation_dataset = val_dataset,\n",
        "                     num_input_channels = 3,\n",
        "                     num_EFs = len(EF_features),\n",
        "                     batch_size = 4,\n",
        "                     image_embedding_architecture = \"ResNet18\",\n",
        "                     image_encoder_lr = 0,\n",
        "                     general_lr = 1e-3,\n",
        "                     output_activation = None,\n",
        "                     loss_function_str = \"weighted_CELoss\",\n",
        "                     num_output_classes = 5,\n",
        "                     lr_scheduler_patience = 3,\n",
        "                     zoom_levels = zoom_levels,\n",
        "                     class_weights = class_weights,\n",
        "                     image_only_model = True,\n",
        "                     weight_decay = 0.001)\n",
        "\n",
        "max_epochs = 5\n",
        "log_every_n_steps = 100 # every 5th batch, log all the metrics\n",
        "\n",
        "\n",
        "early_stop_callback = EarlyStopping(monitor=\"val/loss\", patience=5, mode=\"min\")\n",
        "tic = time.perf_counter()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    trainer = pl.Trainer(max_epochs = max_epochs, accelerator = 'gpu',\n",
        "                         log_every_n_steps = log_every_n_steps,\n",
        "                         callbacks = [early_stop_callback])\n",
        "else:\n",
        "    trainer = pl.Trainer(max_epochs = max_epochs,\n",
        "                         log_every_n_steps = log_every_n_steps,\n",
        "                         callbacks = [early_stop_callback])\n",
        "%reload_ext tensorboard\n",
        "%tensorboard --logdir=lightning_logs/\n",
        "trainer.fit(model)\n",
        "\n",
        "toc = time.perf_counter()\n",
        "display(toc - tic)"
      ],
      "metadata": {
        "id": "kKRomEA0obfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot statistics"
      ],
      "metadata": {
        "id": "xQ6g1k0ixHYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pseudocode for checking model performance\n",
        "\"\"\"\n",
        "model.eval()\n",
        "\n",
        "predictions = model.eval(test_df[EF_features])\n",
        "\n",
        "metric = metric(predictions, test_df[\"damage_class\"])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mkEQX51i-r9g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}